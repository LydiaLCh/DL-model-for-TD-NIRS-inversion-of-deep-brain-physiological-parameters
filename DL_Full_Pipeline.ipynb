{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4502f69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDTOF Deep Learning Pipeline (DL_Full_Pipeline)\\n\\nThis module implements a complete, modular deep learning pipeline for predicting\\nthe optical properties (μa, μs′) of homogeneous tissue models from time-resolved\\nDTOF (Distribution of Time-of-Flight) signals.\\n\\nThe pipeline is designed for reproducible experimentation and supports flexible\\npreprocessing, multi-channel DTOF representations, log-space regression, and\\nsystematic neural architecture comparisons (e.g. baseline vs dilated CNNs).\\n\\n-------------------------------------------------------------------------------\\nApplicability\\n-------------------------------------------------------------------------------\\nThis framework is intended for studies involving:\\n    • Monte Carlo simulated DTOFs (homogeneous tissue models)\\n    • Inversion of DTOFs to estimate optical properties (μa, μs′)\\n    • Benchmarking preprocessing choices (e.g. Savitzky–Golay filtering)\\n    • Deep learning architecture optimisation (kernel size, dilation, channels)\\n    • Comparison of single-, multi-, and hybrid-channel DTOF representations\\n    • Analysis of long-range temporal sensitivity using dilated convolutions\\n\\n-------------------------------------------------------------------------------\\nSupported Input Format\\n-------------------------------------------------------------------------------\\nInputs are provided as MATLAB v7.3 (.mat, HDF5) files containing:\\n    • t : time axis (T,)\\n    • X : DTOF signals (N, T)\\n    • y : labels (N, 2) = [μa, μs′]\\n\\nData are loaded using h5py and converted to NumPy/PyTorch tensors with\\nappropriate MATLAB → NumPy orientation correction.\\n\\n-------------------------------------------------------------------------------\\nSupported Model Variants\\n-------------------------------------------------------------------------------\\nThe pipeline supports dynamic selection of DTOF channel representations.\\nChannel configuration is selected from the configuration dictionary and handled\\nautomatically by DTOFDataset.\\n\\n    (1) Single-channel DTOF:\\n        • Cropped to a fixed temporal window (e.g. 0–5 ns)\\n        • Savitzky–Golay smoothed and clipped\\n        • Input shape: (1, T)\\n\\n    (2) Three-channel temporal bin model:\\n        • Early, mid, late temporal masks\\n          (e.g. 0–0.5 ns, 0.5–4 ns, 4–5 ns)\\n        • Each mask multiplied with the DTOF\\n        • Input shape: (3, T)\\n\\n    (3) Four-channel hybrid model:\\n        • Channel 1: full DTOF\\n        • Channels 2–4: early / mid / late temporal bins\\n        • Input shape: (4, T)\\n\\n-------------------------------------------------------------------------------\\nPipeline Components\\n-------------------------------------------------------------------------------\\n1. Configuration Dictionary\\n   Centralises all experiment-level parameters, including:\\n        • preprocessing settings (SG window/order, clipping, cropping)\\n        • channel representation mode\\n        • CNN architecture parameters (kernel sizes, dilation, channels)\\n        • optimisation settings (learning rate, batch size, epochs)\\n        • early stopping and checkpointing controls\\n\\n2. DTOFDataset\\n   Implements the full preprocessing chain:\\n        • MATLAB v7.3 (.mat) loading via h5py\\n        • time-axis cropping\\n        • Savitzky–Golay smoothing\\n        • negative-value clipping\\n        • optional per-trace normalisation\\n        • dynamic construction of 1, 3, or 4 input channels\\n        • returns (signal, label) pairs where label = [μa, μs′]\\n\\n3. CNN Model (Net)\\n        • 1D convolutional architecture\\n        • optional use of dilated convolutions for enlarged receptive fields\\n        • batch normalisation, ReLU activations, and pooling\\n        • automatic flatten-dimension inference\\n        • regression head outputs log(μa), log(μs′)\\n\\n4. Training Loop\\n        • log-space MSE optimisation\\n        • linear-space RMSE and mean absolute percentage error reporting\\n        • GPU/CPU device handling\\n        • early stopping based on validation loss\\n        • best-model checkpoint saving\\n        • automatic plotting of training and validation loss curves (PNG)\\n\\n5. Evaluation Utilities\\n        • conversion from log-space predictions to linear units\\n        • MAE, RMSE, and percentage error computation\\n        • prediction vs. ground-truth arrays for downstream analysis\\n\\n-------------------------------------------------------------------------------\\nOutputs\\n-------------------------------------------------------------------------------\\nThe pipeline produces:\\n    • trained model checkpoints (.pt / .pth)\\n    • training and validation loss curves (.png)\\n    • RMSE / MAE / percentage error metrics for μa and μs′\\n    • prediction arrays for further visualisation or analysis\\n\\n-------------------------------------------------------------------------------\\nDependencies\\n-------------------------------------------------------------------------------\\nRequired:\\n    • torch\\n    • numpy\\n    • scipy (Savitzky–Golay filtering)\\n    • h5py (MATLAB v7.3 data loading)\\n    • matplotlib (loss curve plotting)\\n\\nOptional:\\n    • seaborn (enhanced visualisation)\\n\\n-------------------------------------------------------------------------------\\nNotes\\n-------------------------------------------------------------------------------\\n• Models are trained in log-space for numerical stability and scale balancing.\\n• Metrics are always reported in original (linear) physical units.\\n• The pipeline is designed for controlled benchmarking of architectural and\\n  preprocessing choices in DTOF inversion tasks.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DTOF Deep Learning Pipeline (DL_Full_Pipeline)\n",
    "\n",
    "This module implements a complete, modular deep learning pipeline for predicting\n",
    "the optical properties (μa, μs′) of homogeneous tissue models from time-resolved\n",
    "DTOF (Distribution of Time-of-Flight) signals.\n",
    "\n",
    "The pipeline is designed for reproducible experimentation and supports flexible\n",
    "preprocessing, multi-channel DTOF representations, log-space regression, and\n",
    "systematic neural architecture comparisons (e.g. baseline vs dilated CNNs).\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Applicability\n",
    "-------------------------------------------------------------------------------\n",
    "This framework is intended for studies involving:\n",
    "    • Monte Carlo simulated DTOFs (homogeneous tissue models)\n",
    "    • Inversion of DTOFs to estimate optical properties (μa, μs′)\n",
    "    • Benchmarking preprocessing choices (e.g. Savitzky–Golay filtering)\n",
    "    • Deep learning architecture optimisation (kernel size, dilation, channels)\n",
    "    • Comparison of single-, multi-, and hybrid-channel DTOF representations\n",
    "    • Analysis of long-range temporal sensitivity using dilated convolutions\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Supported Input Format\n",
    "-------------------------------------------------------------------------------\n",
    "Inputs are provided as MATLAB v7.3 (.mat, HDF5) files containing:\n",
    "    • t : time axis (T,)\n",
    "    • X : DTOF signals (N, T)\n",
    "    • y : labels (N, 2) = [μa, μs′]\n",
    "\n",
    "Data are loaded using h5py and converted to NumPy/PyTorch tensors with\n",
    "appropriate MATLAB → NumPy orientation correction.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Supported Model Variants\n",
    "-------------------------------------------------------------------------------\n",
    "The pipeline supports dynamic selection of DTOF channel representations.\n",
    "Channel configuration is selected from the configuration dictionary and handled\n",
    "automatically by DTOFDataset.\n",
    "\n",
    "    (1) Single-channel DTOF:\n",
    "        • Cropped to a fixed temporal window (e.g. 0–5 ns)\n",
    "        • Savitzky–Golay smoothed and clipped\n",
    "        • Input shape: (1, T)\n",
    "\n",
    "    (2) Three-channel temporal bin model:\n",
    "        • Early, mid, late temporal masks\n",
    "          (e.g. 0–0.5 ns, 0.5–4 ns, 4–5 ns)\n",
    "        • Each mask multiplied with the DTOF\n",
    "        • Input shape: (3, T)\n",
    "\n",
    "    (3) Four-channel hybrid model:\n",
    "        • Channel 1: full DTOF\n",
    "        • Channels 2–4: early / mid / late temporal bins\n",
    "        • Input shape: (4, T)\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Pipeline Components\n",
    "-------------------------------------------------------------------------------\n",
    "1. Configuration Dictionary\n",
    "   Centralises all experiment-level parameters, including:\n",
    "        • preprocessing settings (SG window/order, clipping, cropping)\n",
    "        • channel representation mode\n",
    "        • CNN architecture parameters (kernel sizes, dilation, channels)\n",
    "        • optimisation settings (learning rate, batch size, epochs)\n",
    "        • early stopping and checkpointing controls\n",
    "\n",
    "2. DTOFDataset\n",
    "   Implements the full preprocessing chain:\n",
    "        • MATLAB v7.3 (.mat) loading via h5py\n",
    "        • time-axis cropping\n",
    "        • Savitzky–Golay smoothing\n",
    "        • negative-value clipping\n",
    "        • optional per-trace normalisation\n",
    "        • dynamic construction of 1, 3, or 4 input channels\n",
    "        • returns (signal, label) pairs where label = [μa, μs′]\n",
    "\n",
    "3. CNN Model (Net)\n",
    "        • 1D convolutional architecture\n",
    "        • optional use of dilated convolutions for enlarged receptive fields\n",
    "        • batch normalisation, ReLU activations, and pooling\n",
    "        • automatic flatten-dimension inference\n",
    "        • regression head outputs log(μa), log(μs′)\n",
    "\n",
    "4. Training Loop\n",
    "        • log-space MSE optimisation\n",
    "        • linear-space RMSE and mean absolute percentage error reporting\n",
    "        • GPU/CPU device handling\n",
    "        • early stopping based on validation loss\n",
    "        • best-model checkpoint saving\n",
    "        • automatic plotting of training and validation loss curves (PNG)\n",
    "\n",
    "5. Evaluation Utilities\n",
    "        • conversion from log-space predictions to linear units\n",
    "        • MAE, RMSE, and percentage error computation\n",
    "        • prediction vs. ground-truth arrays for downstream analysis\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Outputs\n",
    "-------------------------------------------------------------------------------\n",
    "The pipeline produces:\n",
    "    • trained model checkpoints (.pt / .pth)\n",
    "    • training and validation loss curves (.png)\n",
    "    • RMSE / MAE / percentage error metrics for μa and μs′\n",
    "    • prediction arrays for further visualisation or analysis\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Dependencies\n",
    "-------------------------------------------------------------------------------\n",
    "Required:\n",
    "    • torch\n",
    "    • numpy\n",
    "    • scipy (Savitzky–Golay filtering)\n",
    "    • h5py (MATLAB v7.3 data loading)\n",
    "    • matplotlib (loss curve plotting)\n",
    "\n",
    "Optional:\n",
    "    • seaborn (enhanced visualisation)\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Notes\n",
    "-------------------------------------------------------------------------------\n",
    "• Models are trained in log-space for numerical stability and scale balancing.\n",
    "• Metrics are always reported in original (linear) physical units.\n",
    "• The pipeline is designed for controlled benchmarking of architectural and\n",
    "  preprocessing choices in DTOF inversion tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb53820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Input (.mat struct containing t, X, y)\n",
    "    \"mat_path\": \"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/dataset_homo_small.mat\",\n",
    "\n",
    "    # Field names inside the .mat struct (safe + explicit)\n",
    "    # If your .mat loads as {\"data\": <struct>}, then mat_struct_key=\"data\"\n",
    "    # If t/X/y are top-level, mat_struct_key can be None.\n",
    "    \"mat_struct_key\": None,     # or e.g. \"dtof\" / \"data\"\n",
    "    \"t_key\": \"t\",\n",
    "    \"X_key\": \"X\",\n",
    "    \"y_key\": \"y\",\n",
    "\n",
    "    # Preprocessing\n",
    "    \"sg_window\": 21,\n",
    "    \"sg_order\": 1,\n",
    "    \"eps\": 1e-8,\n",
    "    \"crop_t_max\": 5.0,          # crop DTOF to 0–5 ns using t\n",
    "\n",
    "    # Channels: \"single\", \"early_mid_late\", \"hybrid_4ch\"\n",
    "    \"channel_mode\": \"single\",\n",
    "\n",
    "    # Data split + loader\n",
    "    \"train_frac\": 0.8,\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    # Model\n",
    "    \"in_channels\": 1,           # will be overwritten based on dataset.C\n",
    "    \"output_dim\": 2,            # mua + mus'\n",
    "    \"hidden_fc\": [128, 64],\n",
    "\n",
    "    # Training\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 20,\n",
    "\n",
    "    # Outputs / logging\n",
    "    \"run_name\": \"exp_single_w21_o1\",\n",
    "    \"save_dir\": \"JSON logs\",\n",
    "    \"log_path\": \"JSON logs/dtof_runs_log.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0153f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DTOFDataset: preprocessing + channel construction \n",
    "\n",
    "class DTOFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DTOF dataset with preprocessing and flexible channel configurations.\n",
    "\n",
    "    Input format (MATLAB .mat):\n",
    "        - t: (T_full,) time axis\n",
    "        - X: (N, T_full) DTOF traces\n",
    "        - y: (N, 2) labels: y[:,0]=mua, y[:,1]=mus'\n",
    "\n",
    "    Preprocessing:\n",
    "        - load .mat\n",
    "        - crop time axis to [0, crop_t_max]\n",
    "        - Savitzky-Golay smoothing\n",
    "        - negative-value clipping\n",
    "        - per-trace standardisation (currently computed, not applied, matching your code)\n",
    "        - channel construction:\n",
    "            * \"single\"        -> 1 channel, full DTOF\n",
    "            * \"early_mid_late\"-> 3 channels (early/mid/late masks)\n",
    "            * \"hybrid_4ch\"    -> 4 channels (full + 3 masks)\n",
    "\n",
    "    Returns:\n",
    "        signal: (C, T) tensor\n",
    "        label:  (2,) tensor [mua, mus']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mat_path: str, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # --- Load .mat ---\n",
    "        with h5py.File(mat_path, \"r\") as f:\n",
    "            # NOTE: MATLAB stores arrays column-major and often transposed\n",
    "            t = np.array(f[cfg.get(\"t_key\", \"t\")]).squeeze()\n",
    "            X = np.array(f[cfg.get(\"X_key\", \"X\")])\n",
    "            y = np.array(f[cfg.get(\"y_key\", \"y\")])\n",
    "        \n",
    "        # MATLAB → NumPy orientation fixes\n",
    "        if X.shape[0] == t.shape[0]:\n",
    "            X = X.T   # (T, N) → (N, T)\n",
    "\n",
    "        if y.shape[0] == 2:\n",
    "            y = y.T   # (2, N) → (N, 2)\n",
    "\n",
    "        # --- Crop time axis ---\n",
    "        t_mask = (t >= 0.0) & (t <= cfg[\"crop_t_max\"])\n",
    "        time = t[t_mask]            # (T,)\n",
    "        dtof = X[:, t_mask]         # (N, T)\n",
    "        labels = y                  # (N, 2)\n",
    "\n",
    "        # --- Savitzky–Golay smoothing ---\n",
    "        dtof_smooth = savgol_filter(\n",
    "            dtof,\n",
    "            cfg[\"sg_window\"],\n",
    "            cfg[\"sg_order\"],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # --- Clip negatives and standardise ---\n",
    "        eps = cfg[\"eps\"]\n",
    "        dtof_smooth[dtof_smooth < 0] = eps\n",
    "\n",
    "        mean = dtof_smooth.mean(axis=1, keepdims=True)\n",
    "        std = dtof_smooth.std(axis=1, keepdims=True)\n",
    "        # dtof_std = (dtof_smooth - mean) / (std + eps)  # (N, T)\n",
    "\n",
    "        # --- Build channels ---\n",
    "        channels = self.build_channels(time, dtof_smooth, cfg[\"channel_mode\"])\n",
    "        # channels: (N, C, T)\n",
    "\n",
    "        self.signals = torch.tensor(channels, dtype=torch.float32)  # (N, C, T)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)     # (N, 2)\n",
    "\n",
    "        self.N, self.C, self.T = self.signals.shape\n",
    "\n",
    "    def build_channels(self, t: np.ndarray, dtof: np.ndarray, mode: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct channels based on the chosen mode:\n",
    "            \"single\"         -> 1 channel, full DTOF\n",
    "            \"early_mid_late\" -> 3 masked channels\n",
    "            \"hybrid_4ch\"     -> 1 full + 3 masked = 4 channels\n",
    "        \"\"\"\n",
    "        N, T = dtof.shape\n",
    "\n",
    "        if mode == \"single\":\n",
    "            return dtof[:, None, :]  # (N,1,T)\n",
    "\n",
    "        early = ((t >= 0.0) & (t < 0.5)).astype(float)\n",
    "        mid  = ((t >= 0.5) & (t < 4.0)).astype(float)\n",
    "        late = ((t >= 4.0) & (t <= self.cfg[\"crop_t_max\"])).astype(float)\n",
    "        masks = np.stack([early, mid, late], axis=0)  # (3,T)\n",
    "\n",
    "        if mode == \"early_mid_late\":\n",
    "            return dtof[:, None, :] * masks[None, :, :]  # (N,3,T)\n",
    "\n",
    "        if mode == \"hybrid_4ch\":\n",
    "            ch_full = dtof[:, None, :]                    # (N,1,T)\n",
    "            ch_bins = dtof[:, None, :] * masks[None, :, :]  # (N,3,T)\n",
    "            return np.concatenate([ch_full, ch_bins], axis=1)  # (N,4,T)\n",
    "\n",
    "        raise ValueError(f\"Unknown channel_mode: {mode}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.signals[idx], self.labels[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67272cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CNN Model (Net) with flexible in_channels and FC head \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN for DTOF-based regression to (μa, μs').\n",
    "\n",
    "    - Supports variable input channels (C) from CONFIG[\"in_channels\"]\n",
    "    - Uses three conv blocks with BatchNorm, ReLU, MaxPool\n",
    "    - Automatically computes flatten dimension\n",
    "    - FC layers controlled by CONFIG[\"hidden_fc\"]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict, input_length: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        C = cfg[\"in_channels\"]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(C, 32, kernel_size=3, padding=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 16, kernel_size=7, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Determine flatten dim automatically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, C, input_length)  # (1,C,T)\n",
    "            feat = self._forward_features(dummy)\n",
    "            flatten_dim = feat.shape[1]  # (1, flatten_dim)\n",
    "\n",
    "        # Build FC head from cfg[\"hidden_fc\"]\n",
    "        fc_layers = []\n",
    "        last = flatten_dim\n",
    "        for h in cfg[\"hidden_fc\"]:\n",
    "            fc_layers += [nn.Linear(last, h), nn.ReLU()]\n",
    "            last = h\n",
    "\n",
    "        fc_layers.append(nn.Linear(last, cfg[\"output_dim\"]))\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def _forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.flatten(1)  # (batch, -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5481a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training loop with loss tracking, prediction inspection & plotting \n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    cfg: dict,\n",
    "    device: torch.device\n",
    "):\n",
    "    num_epochs = cfg[\"epochs\"]\n",
    "    save_dir   = cfg[\"save_dir\"]\n",
    "    run_name   = cfg[\"run_name\"]\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses   = []\n",
    "    best_val     = float(\"inf\")\n",
    "    best_path    = os.path.join(save_dir, f\"{run_name}_best.pth\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ----- TRAIN -----\n",
    "        model.train()\n",
    "        running_train = 0.0\n",
    "        n_train = 0\n",
    "\n",
    "        for signals, labels in train_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels  = labels.to(device).float()\n",
    "            B = labels.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(signals)\n",
    "            loss  = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train += loss.item() * B\n",
    "            n_train += B\n",
    "\n",
    "        epoch_train = running_train / max(1, n_train)\n",
    "        train_losses.append(epoch_train)\n",
    "\n",
    "        # ----- VALIDATION -----\n",
    "        model.eval()\n",
    "        running_val = 0.0\n",
    "        n_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (signals, labels) in enumerate(val_loader):\n",
    "                signals = signals.to(device)\n",
    "                labels  = labels.to(device).float()\n",
    "                B = labels.size(0)\n",
    "\n",
    "                preds = model(signals)\n",
    "                loss  = loss_fn(preds, labels)\n",
    "                running_val += loss.item() * B\n",
    "                n_val += B\n",
    "\n",
    "                if epoch == 0 and batch_idx == 0:\n",
    "                    print(\"\\n[VAL SAMPLE] First batch predictions vs labels:\")\n",
    "                    print(\"Preds (μa, μs'):\\n\", preds[:5].cpu())\n",
    "                    print(\"Labels (μa, μs'):\\n\", labels[:5].cpu())\n",
    "                    print(\"Abs error:\\n\", (preds[:5] - labels[:5]).abs().cpu())\n",
    "                    print(\"---------------------------------------------------\")\n",
    "\n",
    "        epoch_val = running_val / max(1, n_val)\n",
    "        val_losses.append(epoch_val)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train MSE: {epoch_train:.4f} | Val MSE: {epoch_val:.4f}\")\n",
    "\n",
    "        if epoch_val < best_val:\n",
    "            best_val = epoch_val\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "    # ----- Plot -----\n",
    "    fig_path = os.path.join(save_dir, f\"{run_name}_loss_curves.png\")\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train MSE\")\n",
    "    plt.plot(val_losses,   label=\"Val MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.title(f\"MSE vs. Epoch: {run_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\":   val_losses,\n",
    "        \"best_val\":     best_val,\n",
    "        \"best_path\":    best_path,\n",
    "        \"loss_plot\":    fig_path,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5699a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Evaluation and visualisation of performance (MAE / RMSE, percentage error)\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on a DataLoader and compute MAE/RMSE for (μa, μs'),\n",
    "    plus percentage error plots vs actual values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate(self, data_loader: DataLoader, cfg: dict):\n",
    "        all_preds  = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in data_loader:\n",
    "                signals = signals.to(self.device)\n",
    "                labels  = labels.to(self.device).float()\n",
    "\n",
    "                preds = self.model(signals)\n",
    "\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        all_preds  = torch.cat(all_preds,  dim=0)  # (N,2)\n",
    "        all_labels = torch.cat(all_labels, dim=0)  # (N,2)\n",
    "\n",
    "        # Basic metrics: MAE and RMSE\n",
    "        abs_err = torch.abs(all_preds - all_labels)\n",
    "        sq_err  = (all_preds - all_labels) ** 2\n",
    "\n",
    "        mae  = abs_err.mean(dim=0)             # (2,)\n",
    "        rmse = torch.sqrt(sq_err.mean(dim=0))  # (2,)\n",
    "\n",
    "        # ---------- Percentage error vs Actual ----------\n",
    "        preds_np  = all_preds.detach().numpy()\n",
    "        labels_np = all_labels.detach().numpy()\n",
    "\n",
    "        # Avoid division by very small numbers\n",
    "        eps = cfg.get(\"eps\", 1e-12)\n",
    "        denom = np.maximum(np.abs(labels_np), eps)  # (N,2)\n",
    "\n",
    "        pct_error = 100.0 * (preds_np - labels_np) / denom  # signed %\n",
    "        abs_pct_error = np.abs(pct_error)                   # absolute %\n",
    "\n",
    "        # Scatter plots: Actual vs % error for μa and μs′\n",
    "        save_dir = cfg[\"save_dir\"]\n",
    "        run_name = cfg[\"run_name\"]\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # error vs true μa plot\n",
    "        fig_mua = os.path.join(save_dir, f\"{run_name}_pct_error_mua.png\")\n",
    "        x = labels_np[:, 0]            # true μa\n",
    "        y = abs_pct_error[:, 0]        # absolute percentage error\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(x, y, s=10, alpha=0.6, label=\"Absolute % error\")\n",
    "\n",
    "        # Exponential model\n",
    "        def _exp_model(x, a, b, c):\n",
    "            return a * np.exp(-b * x) + c\n",
    "\n",
    "        # Fit curve\n",
    "        try:\n",
    "            popt, _ = curve_fit(_exp_model, x, y, p0=[100, 1.0, 0.0], maxfev=5000)\n",
    "            x_fit = np.linspace(min(x), max(x), 300)\n",
    "            y_fit = _exp_model(x_fit, *popt)\n",
    "            plt.plot(\n",
    "                x_fit, y_fit, \"r-\", linewidth=2,\n",
    "                label=f\"Fit: a·exp(-b·x) + c\\n\"\n",
    "                    f\"a={popt[0]:.2f}, b={popt[1]:.2f}, c={popt[2]:.2f}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] μa exponential fit failed:\", e)\n",
    "\n",
    "        plt.xlabel(\"True μa\")\n",
    "        plt.ylabel(\"Absolute % error\")\n",
    "        plt.title(f\"Percentage error vs Actual μa: {run_name}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_mua, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # error vs true μs′ plot\n",
    "        fig_mus = os.path.join(save_dir, f\"{run_name}_pct_error_mus.png\")\n",
    "        x = labels_np[:, 1]            # true μs'\n",
    "        y = abs_pct_error[:, 1]        # absolute percentage error\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(x, y, s=10, alpha=0.6, label=\"Absolute % error\")\n",
    "\n",
    "        # Exponential model\n",
    "        def _exp_model(x, a, b, c):\n",
    "            return a * np.exp(-b * x) + c\n",
    "\n",
    "        # Fit curve\n",
    "        try:\n",
    "            popt, _ = curve_fit(_exp_model, x, y, p0=[100, 0.5, 0.0], maxfev=5000)\n",
    "            x_fit = np.linspace(min(x), max(x), 300)\n",
    "            y_fit = _exp_model(x_fit, *popt)\n",
    "            plt.plot(\n",
    "                x_fit, y_fit, \"r-\", linewidth=2,\n",
    "                label=f\"Fit: a·e^(-b·x) + c\\n\"\n",
    "                    f\"a={popt[0]:.2f}, b={popt[1]:.2f}, c={popt[2]:.2f}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] μs' exponential fit failed:\", e)\n",
    "\n",
    "        plt.xlabel(\"True μs'\")\n",
    "        plt.ylabel(\"Absolute % error\")\n",
    "        plt.title(f\"Percentage error vs Actual μs': {run_name}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_mus, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        metrics = {\n",
    "            \"MAE\": mae.numpy(),          # [MAE_mua, MAE_mus]\n",
    "            \"RMSE\": rmse.numpy(),        # [RMSE_mua, RMSE_mus]\n",
    "            \"preds\": preds_np,\n",
    "            \"labels\": labels_np,\n",
    "            \"pct_error\": pct_error,      # signed %\n",
    "            \"abs_pct_error\": abs_pct_error,\n",
    "            \"pct_error_plots\": {\n",
    "                \"mua\": fig_mua,\n",
    "                \"mus\": fig_mus,\n",
    "            }\n",
    "        }\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d56a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Logging of runs (config + metrics) to JSON \n",
    "\n",
    "def log_run(cfg, results, log_path):\n",
    "    \"\"\"\n",
    "    Append a run entry to a JSON log file, including config and metrics.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "    # Handle both old ('best_path') and new ('model_path') keys safely\n",
    "    model_path = results.get(\"model_path\", results.get(\"best_path\", None))\n",
    "\n",
    "    entry = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"run_name\": cfg[\"run_name\"],\n",
    "\n",
    "    # ---- DATA SOURCE ----\n",
    "    \"data_type\": \"mat\",\n",
    "    \"mat_path\": cfg.get(\"mat_path\", None),\n",
    "    \"mat_struct_key\": cfg.get(\"mat_struct_key\", None),\n",
    "    \"t_key\": cfg.get(\"t_key\", \"t\"),\n",
    "    \"X_key\": cfg.get(\"X_key\", \"X\"),\n",
    "    \"y_key\": cfg.get(\"y_key\", \"y\"),\n",
    "\n",
    "    # ---- PREPROCESSING / MODEL ----\n",
    "    \"sg_window\": cfg[\"sg_window\"],\n",
    "    \"sg_order\": cfg[\"sg_order\"],\n",
    "    \"channel_mode\": cfg[\"channel_mode\"],\n",
    "    \"crop_t_max\": cfg[\"crop_t_max\"],\n",
    "    \"hidden_fc\": cfg[\"hidden_fc\"],\n",
    "    \"lr\": cfg[\"lr\"],\n",
    "    \"epochs\": cfg[\"epochs\"],\n",
    "    \"batch_size\": cfg[\"batch_size\"],\n",
    "    \"best_val\": results[\"best_val\"],\n",
    "    \"loss_plot\": results[\"loss_plot\"],\n",
    "    \"model_path\": model_path,\n",
    "}\n",
    "\n",
    "    # to store MAE / RMSE \n",
    "    if \"val_metrics\" in results:\n",
    "        entry[\"MAE\"] = results[\"val_metrics\"][\"MAE\"].tolist()\n",
    "        entry[\"RMSE\"] = results[\"val_metrics\"][\"RMSE\"].tolist()\n",
    "        entry[\"pct_error_plots\"] = results[\"val_metrics\"][\"pct_error_plots\"]\n",
    "\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        data.append(entry)\n",
    "    else:\n",
    "        data = [entry]\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3e79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_channels(mode: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns number of input channels for the CNN depending on the preprocessing mode.\n",
    "\n",
    "    mode options:\n",
    "        \"single\"          -> 1 channel  (raw DTOF only)\n",
    "        \"early_mid_late\"  -> 3 channels (masked temporal bins)\n",
    "        \"hybrid_4ch\"      -> 4 channels (raw + 3 temporal bins)\n",
    "    \"\"\"\n",
    "    if mode == \"single\":\n",
    "        return 1\n",
    "    elif mode == \"early_mid_late\":\n",
    "        return 3\n",
    "    elif mode == \"hybrid_4ch\":\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown channel_mode: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228a0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Single experiment runner\n",
    "\n",
    "def run_experiment(cfg):\n",
    "\n",
    "    cfg = dict(cfg)\n",
    "\n",
    "    # 1. Build dataset + loaders  (MAT-based)\n",
    "    dataset = DTOFDataset(\n",
    "        mat_path=cfg[\"mat_path\"],\n",
    "        cfg=cfg,\n",
    "    )\n",
    "\n",
    "    n_train = int(cfg[\"train_frac\"] * len(dataset))\n",
    "    n_val = len(dataset) - n_train\n",
    "    train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # 2. Build model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Derive in_channels from the dataset (most reliable)\n",
    "    cfg[\"in_channels\"] = dataset.C\n",
    "\n",
    "    model = Net(\n",
    "        cfg=cfg,\n",
    "        input_length=dataset.T,\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss + optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "\n",
    "    # 3. Training Loop\n",
    "    best_val = float(\"inf\")\n",
    "    save_path = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    os.makedirs(\"Best paths\", exist_ok=True)\n",
    "    os.makedirs(\"Model evaluation figs\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for signals, labels in train_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(signals)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_running = 0.0\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in val_loader:\n",
    "                signals = signals.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                preds = model(signals)\n",
    "                val_running += loss_fn(preds, labels).item()\n",
    "\n",
    "        val_loss = val_running / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ---- Save best model ----\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            save_path = f\"Best paths/{cfg['run_name']}_best.pth\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # ---- Plot curves for this run ----\n",
    "    fig_path = f\"Model evaluation figs/{cfg['run_name']}_loss_curves.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train MSE\")\n",
    "    plt.plot(val_losses, label=\"Val MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(f\"Loss Curves: {cfg['run_name']}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ---- Evaluate best model on validation set with ModelEvaluator ----\n",
    "    best_model = Net(cfg=cfg, input_length=dataset.T).to(device)\n",
    "\n",
    "    if save_path is not None:\n",
    "        best_model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "\n",
    "    evaluator = ModelEvaluator(model=best_model, device=device)\n",
    "\n",
    "    eval_cfg = {\n",
    "        \"run_name\": cfg[\"run_name\"],\n",
    "        \"save_dir\": cfg.get(\"eval_save_dir\", \"Model evaluation figs\"),\n",
    "        \"eps\": cfg.get(\"eps\", 1e-8),\n",
    "    }\n",
    "\n",
    "    val_metrics = evaluator.evaluate(val_loader, cfg=eval_cfg)\n",
    "\n",
    "    return {\n",
    "        \"run_name\": cfg[\"run_name\"],\n",
    "        \"best_val\": best_val,\n",
    "        \"model_path\": save_path,\n",
    "        \"cfg\": cfg,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"loss_plot\": fig_path,\n",
    "        \"val_metrics\": val_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf7abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Grid search over a few configurations \n",
    "\n",
    "def grid_search():\n",
    "    base_cfg = CONFIG\n",
    "\n",
    "    windows = [11, 21, 31, 41]\n",
    "    orders  = [1,  2,  3,  4]\n",
    "    modes   = [\"single\", \"early_mid_late\", \"hybrid_4ch\"]\n",
    "    lrs     = [1e-3, 3e-4]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for w in windows:\n",
    "        for o in orders:\n",
    "            if w <= o:\n",
    "                continue  # SG requires window_length > polyorder\n",
    "            for mode in modes:\n",
    "                for lr in lrs:\n",
    "                    cfg = dict(base_cfg)\n",
    "                    cfg[\"sg_window\"]    = w\n",
    "                    cfg[\"sg_order\"]     = o\n",
    "                    cfg[\"channel_mode\"] = mode\n",
    "                    cfg[\"lr\"]           = lr\n",
    "\n",
    "                    lr_tag = f\"{lr:.0e}\"\n",
    "                    cfg[\"run_name\"] = f\"{mode}_w{w}_o{o}_lr{lr_tag}\"\n",
    "\n",
    "                    print(f\"\\n=== Running {cfg['run_name']} ===\")\n",
    "                    res = run_experiment(cfg)\n",
    "\n",
    "                    results.append(res)\n",
    "\n",
    "                    # log run to JSON (should now include mat_path in log_run)\n",
    "                    log_run(cfg, res, cfg[\"log_path\"])\n",
    "\n",
    "    results_sorted = sorted(results, key=lambda x: x[\"best_val\"])\n",
    "\n",
    "    print(\"\\n==================== GRID SEARCH SUMMARY ====================\")\n",
    "    for r in results_sorted[:10]:\n",
    "        best_val = r[\"best_val\"]\n",
    "        mae      = r[\"val_metrics\"][\"MAE\"]\n",
    "        rmse     = r[\"val_metrics\"][\"RMSE\"]\n",
    "\n",
    "        print(\n",
    "            f\"{r['run_name']} | \"\n",
    "            f\"val_MSE={best_val:.6f} | \"\n",
    "            f\"MAE=(μa={mae[0]:.4f}, μs'={mae[1]:.4f}) | \"\n",
    "            f\"RMSE=(μa={rmse[0]:.4f}, μs'={rmse[1]:.4f})\"\n",
    "        )\n",
    "\n",
    "    best = results_sorted[0]\n",
    "    print(\"\\nBEST MODEL (by val MSE):\")\n",
    "    print(best[\"run_name\"])\n",
    "    print(f\"  val_MSE = {best['best_val']:.6f}\")\n",
    "    print(f\"  model_path = {best['model_path']}\")\n",
    "    print(f\"  loss_plot  = {best['loss_plot']}\")\n",
    "    print(f\"  MAE  = {best['val_metrics']['MAE']}\")\n",
    "    print(f\"  RMSE = {best['val_metrics']['RMSE']}\")\n",
    "\n",
    "    return results_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8717d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_single_w21_o1 12.582507729530334\n"
     ]
    }
   ],
   "source": [
    "# Single experiment run \n",
    "if __name__ == \"__main__\":\n",
    "    run_info = run_experiment(CONFIG)\n",
    "    print(run_info[\"run_name\"], run_info[\"best_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db27c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running single_w11_o1_lr1e-03 ===\n",
      "\n",
      "=== Running single_w11_o1_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o1_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o1_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o1_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o1_lr3e-04 ===\n",
      "\n",
      "=== Running single_w11_o2_lr1e-03 ===\n",
      "\n",
      "=== Running single_w11_o2_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o2_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o2_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o2_lr1e-03 ===\n",
      "[WARN] μs' exponential fit failed: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n",
      "\n",
      "=== Running hybrid_4ch_w11_o2_lr3e-04 ===\n",
      "\n",
      "=== Running single_w11_o3_lr1e-03 ===\n",
      "\n",
      "=== Running single_w11_o3_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o3_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o3_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o3_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o3_lr3e-04 ===\n",
      "\n",
      "=== Running single_w11_o4_lr1e-03 ===\n",
      "\n",
      "=== Running single_w11_o4_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o4_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w11_o4_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o4_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w11_o4_lr3e-04 ===\n",
      "\n",
      "=== Running single_w21_o1_lr1e-03 ===\n",
      "\n",
      "=== Running single_w21_o1_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o1_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o1_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o1_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o1_lr3e-04 ===\n",
      "\n",
      "=== Running single_w21_o2_lr1e-03 ===\n",
      "\n",
      "=== Running single_w21_o2_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o2_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o2_lr3e-04 ===\n",
      "[WARN] μs' exponential fit failed: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n",
      "\n",
      "=== Running hybrid_4ch_w21_o2_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o2_lr3e-04 ===\n",
      "\n",
      "=== Running single_w21_o3_lr1e-03 ===\n",
      "\n",
      "=== Running single_w21_o3_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o3_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o3_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o3_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o3_lr3e-04 ===\n",
      "\n",
      "=== Running single_w21_o4_lr1e-03 ===\n",
      "\n",
      "=== Running single_w21_o4_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o4_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w21_o4_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o4_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w21_o4_lr3e-04 ===\n",
      "\n",
      "=== Running single_w31_o1_lr1e-03 ===\n",
      "\n",
      "=== Running single_w31_o1_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o1_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o1_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o1_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o1_lr3e-04 ===\n",
      "\n",
      "=== Running single_w31_o2_lr1e-03 ===\n",
      "\n",
      "=== Running single_w31_o2_lr3e-04 ===\n",
      "[WARN] μs' exponential fit failed: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n",
      "\n",
      "=== Running early_mid_late_w31_o2_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o2_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o2_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o2_lr3e-04 ===\n",
      "\n",
      "=== Running single_w31_o3_lr1e-03 ===\n",
      "[WARN] μs' exponential fit failed: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n",
      "\n",
      "=== Running single_w31_o3_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o3_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o3_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o3_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o3_lr3e-04 ===\n",
      "\n",
      "=== Running single_w31_o4_lr1e-03 ===\n",
      "\n",
      "=== Running single_w31_o4_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o4_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w31_o4_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o4_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w31_o4_lr3e-04 ===\n",
      "\n",
      "=== Running single_w41_o1_lr1e-03 ===\n",
      "\n",
      "=== Running single_w41_o1_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o1_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o1_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o1_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o1_lr3e-04 ===\n",
      "\n",
      "=== Running single_w41_o2_lr1e-03 ===\n",
      "\n",
      "=== Running single_w41_o2_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o2_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o2_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o2_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o2_lr3e-04 ===\n",
      "\n",
      "=== Running single_w41_o3_lr1e-03 ===\n",
      "\n",
      "=== Running single_w41_o3_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o3_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o3_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o3_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o3_lr3e-04 ===\n",
      "\n",
      "=== Running single_w41_o4_lr1e-03 ===\n",
      "\n",
      "=== Running single_w41_o4_lr3e-04 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o4_lr1e-03 ===\n",
      "\n",
      "=== Running early_mid_late_w41_o4_lr3e-04 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o4_lr1e-03 ===\n",
      "\n",
      "=== Running hybrid_4ch_w41_o4_lr3e-04 ===\n",
      "\n",
      "==================== GRID SEARCH SUMMARY ====================\n",
      "hybrid_4ch_w31_o1_lr1e-03 | val_MSE=6.608581 | MAE=(μa=0.0822, μs'=2.3162) | RMSE=(μa=0.1015, μs'=4.0836)\n",
      "hybrid_4ch_w41_o3_lr1e-03 | val_MSE=7.167460 | MAE=(μa=0.0799, μs'=2.4114) | RMSE=(μa=0.0990, μs'=4.2472)\n",
      "hybrid_4ch_w11_o2_lr3e-04 | val_MSE=7.277138 | MAE=(μa=0.0567, μs'=2.4673) | RMSE=(μa=0.0687, μs'=4.2970)\n",
      "single_w21_o4_lr1e-03 | val_MSE=7.746948 | MAE=(μa=0.1044, μs'=1.9880) | RMSE=(μa=0.1290, μs'=3.8634)\n",
      "early_mid_late_w11_o1_lr3e-04 | val_MSE=7.834634 | MAE=(μa=0.0468, μs'=2.4664) | RMSE=(μa=0.0620, μs'=4.4702)\n",
      "single_w41_o2_lr1e-03 | val_MSE=7.985149 | MAE=(μa=0.1027, μs'=2.6254) | RMSE=(μa=0.1361, μs'=4.3950)\n",
      "hybrid_4ch_w41_o1_lr1e-03 | val_MSE=8.249351 | MAE=(μa=0.0995, μs'=2.5905) | RMSE=(μa=0.1223, μs'=4.5550)\n",
      "single_w41_o1_lr1e-03 | val_MSE=9.170858 | MAE=(μa=0.1080, μs'=2.9018) | RMSE=(μa=0.1497, μs'=4.8168)\n",
      "hybrid_4ch_w21_o4_lr3e-04 | val_MSE=9.194666 | MAE=(μa=0.0574, μs'=2.6507) | RMSE=(μa=0.0656, μs'=4.6468)\n",
      "early_mid_late_w41_o2_lr3e-04 | val_MSE=9.574369 | MAE=(μa=0.0735, μs'=2.6467) | RMSE=(μa=0.0868, μs'=4.9312)\n",
      "\n",
      "BEST MODEL (by val MSE):\n",
      "hybrid_4ch_w31_o1_lr1e-03\n",
      "  val_MSE = 6.608581\n",
      "  model_path = Best paths/hybrid_4ch_w31_o1_lr1e-03_best.pth\n",
      "  loss_plot  = Model evaluation figs/hybrid_4ch_w31_o1_lr1e-03_loss_curves.png\n",
      "  MAE  = [0.08220965 2.316178  ]\n",
      "  RMSE = [0.10149409 4.083555  ]\n",
      "\n",
      "BEST MODEL FROM GRID SEARCH:\n",
      "hybrid_4ch_w31_o1_lr1e-03 6.60858091711998\n"
     ]
    }
   ],
   "source": [
    "# Run grid search \n",
    "if __name__ == \"__main__\": \n",
    "    results = grid_search()\n",
    "\n",
    "    # print the best configuration \n",
    "    best = min(results, key=lambda r: r[\"best_val\"])\n",
    "    print(\"\\nBEST MODEL FROM GRID SEARCH:\")\n",
    "    print(best[\"run_name\"], best[\"best_val\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
