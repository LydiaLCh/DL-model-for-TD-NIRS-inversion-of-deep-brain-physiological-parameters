{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47b73156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import random \n",
    "import torch \n",
    "from torch import nn \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader,Subset\n",
    "from scipy.signal import savgol_filter \n",
    "import h5py\n",
    "random.seed(0)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f18ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Savitzky Golay filter parameters \n",
    "order = 1\n",
    "frame_length = 21\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b40d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b358256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTOFDataset(Dataset):\n",
    "    \"\"\"\n",
    "        DTOF dataset for MATLAB v7.3 (.mat, HDF5) files.\n",
    "\n",
    "        Required datasets inside .mat:\n",
    "            X : (Nt, N) or (N, Nt)  reflectance DTOFs\n",
    "            y : (2, N)  or (N, 2)   [mua, mus']\n",
    "            t : (Nt,)              time vector in seconds (~1e-12 resolution)\n",
    "\n",
    "        Preprocessing:\n",
    "            - convert t from seconds -> ns\n",
    "            - crop [0, crop_t_max] ns\n",
    "            - Savitzky–Golay smoothing\n",
    "            - clip small values to eps\n",
    "            - input representation:\n",
    "                * \"raw\"      -> use reflectance (smoothed+clipped)\n",
    "                * \"log\"      -> use log(reflectance)\n",
    "                * \"raw_log\"  -> concatenate raw and log along channel dimension\n",
    "            - channel construction:\n",
    "                * \"single\"         -> 1 channel (full)\n",
    "                * \"early_mid_late\" -> 3 channels (early/mid/late masks)\n",
    "                * \"hybrid_4ch\"     -> 4 channels (full + early/mid/late masks)\n",
    "\n",
    "        Returns:\n",
    "            signal: (C, T) float32 tensor\n",
    "            label:  (2,) float32 tensor [mua, mus']  (raw labels for now)\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, mat_path: str, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # ---------- load HDF5 (.mat v7.3) ----------\n",
    "        with h5py.File(mat_path, \"r\") as f:\n",
    "            X = np.array(f[\"X\"], dtype=np.float32)\n",
    "            y = np.array(f[\"y\"], dtype=np.float32)\n",
    "            t = np.array(f[\"t\"], dtype=np.float32).squeeze()\n",
    "\n",
    "        # ---------- normalise shapes ----------\n",
    "        # X -> (N, Nt)\n",
    "        if X.shape[0] == t.shape[0]:\n",
    "            X = X.T\n",
    "\n",
    "        # y -> (N, 2)\n",
    "        if y.shape[0] == 2:\n",
    "            y = y.T\n",
    "\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(f\"Expected X to be 2D, got {X.shape}\")\n",
    "        if y.ndim != 2 or y.shape[1] != 2:\n",
    "            raise ValueError(f\"Expected y to be (N,2), got {y.shape}\")\n",
    "        if t.ndim != 1:\n",
    "            raise ValueError(f\"Expected t to be (Nt,), got {t.shape}\")\n",
    "        if X.shape[1] != t.shape[0]:\n",
    "            raise ValueError(f\"X and t mismatch: X Nt={X.shape[1]} vs t Nt={t.shape[0]}\")\n",
    "\n",
    "        # ---------- time: seconds -> ns ----------\n",
    "        t_ns = t * 1e9\n",
    "\n",
    "        # ---------- crop ----------\n",
    "        crop_t_max = float(cfg[\"crop_t_max\"])  # ns\n",
    "        t_mask = (t_ns >= 0.0) & (t_ns <= crop_t_max)\n",
    "        if not np.any(t_mask):\n",
    "            raise ValueError(\n",
    "                f\"Cropping removed all points. \"\n",
    "                f\"t_ns range=[{t_ns.min():.3g}, {t_ns.max():.3g}] ns, crop_t_max={crop_t_max}\"\n",
    "            )\n",
    "\n",
    "        t_ns = t_ns[t_mask]              # (T,)\n",
    "        dtof = X[:, t_mask]              # (N,T)\n",
    "\n",
    "        N, T = dtof.shape\n",
    "\n",
    "        # ---------- Savitzky–Golay ----------\n",
    "        sg_window = int(cfg[\"sg_window\"])\n",
    "        sg_order = int(cfg[\"sg_order\"])\n",
    "\n",
    "        # enforce validity (odd, > order, <= T)\n",
    "        if sg_window % 2 == 0:\n",
    "            sg_window += 1\n",
    "        if sg_window <= sg_order:\n",
    "            sg_window = sg_order + 2\n",
    "            if sg_window % 2 == 0:\n",
    "                sg_window += 1\n",
    "        if sg_window > T:\n",
    "            sg_window = T if (T % 2 == 1) else (T - 1)\n",
    "\n",
    "        if sg_window >= 3:\n",
    "            dtof = savgol_filter(dtof, sg_window, sg_order, axis=1)\n",
    "\n",
    "        # ---------- clip ----------\n",
    "        eps = float(cfg.get(\"eps\", 1e-12))\n",
    "        dtof[dtof < eps] = eps\n",
    "\n",
    "        # ---------- choose representation ----------\n",
    "        input_rep = cfg.get(\"input_rep\", \"log\")  # \"raw\" | \"log\" | \"raw_log\"\n",
    "\n",
    "        dtof_raw = dtof.astype(np.float32)\n",
    "        dtof_log = np.log(dtof_raw).astype(np.float32)\n",
    "\n",
    "        # ---------- build channels ----------\n",
    "        if input_rep == \"raw\":\n",
    "            channels = self.build_channels(t_ns, dtof_raw, cfg[\"channel_mode\"])  # (N,C,T)\n",
    "\n",
    "        elif input_rep == \"log\":\n",
    "            channels = self.build_channels(t_ns, dtof_log, cfg[\"channel_mode\"])  # (N,C,T)\n",
    "\n",
    "        elif input_rep == \"raw_log\":\n",
    "            ch_raw = self.build_channels(t_ns, dtof_raw, cfg[\"channel_mode\"])    # (N,C,T)\n",
    "            ch_log = self.build_channels(t_ns, dtof_log, cfg[\"channel_mode\"])    # (N,C,T)\n",
    "            channels = np.concatenate([ch_raw, ch_log], axis=1)                  # (N,2C,T)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input_rep: {input_rep}\")\n",
    "\n",
    "        # ---------- to torch ----------\n",
    "        self.signals = torch.tensor(channels, dtype=torch.float32)  # (N,C,T)\n",
    "        self.labels = torch.tensor(y, dtype=torch.float32)          # (N,2)\n",
    "\n",
    "        self.N, self.C, self.T = self.signals.shape\n",
    "\n",
    "    def build_channels(self, t_ns: np.ndarray, dtof: np.ndarray, mode: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Channel masks in ns:\n",
    "            early: 0–0.5 ns\n",
    "            mid:   0.5–4 ns\n",
    "            late:  4–crop_t_max ns\n",
    "        \"\"\"\n",
    "        N, T = dtof.shape\n",
    "        crop_t_max = float(self.cfg[\"crop_t_max\"])\n",
    "\n",
    "        if mode == \"single\":\n",
    "            return dtof[:, None, :]  # (N,1,T)\n",
    "\n",
    "        early = ((t_ns >= 0.0) & (t_ns < 0.5)).astype(np.float32)\n",
    "        mid   = ((t_ns >= 0.5) & (t_ns < 4.0)).astype(np.float32)\n",
    "        late  = ((t_ns >= 4.0) & (t_ns <= crop_t_max)).astype(np.float32)\n",
    "\n",
    "        masks = np.stack([early, mid, late], axis=0)  # (3,T)\n",
    "\n",
    "        if mode == \"early_mid_late\":\n",
    "            return dtof[:, None, :] * masks[None, :, :]  # (N,3,T)\n",
    "\n",
    "        if mode == \"hybrid_4ch\":\n",
    "            full = dtof[:, None, :]                         # (N,1,T)\n",
    "            gated = dtof[:, None, :] * masks[None, :, :]    # (N,3,T)\n",
    "            return np.concatenate([full, gated], axis=1)    # (N,4,T)\n",
    "\n",
    "        raise ValueError(f\"Unknown channel_mode: {mode}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.signals[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e584e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for 1D DTOF signals with flexible input channels.\n",
    "\n",
    "    Channel counts (C) depend on:\n",
    "        channel_mode:\n",
    "            - \"single\"         -> 1\n",
    "            - \"early_mid_late\" -> 3\n",
    "            - \"hybrid_4ch\"     -> 4\n",
    "        input_rep:\n",
    "            - \"raw\" / \"log\"    -> multiplier 1\n",
    "            - \"raw_log\"        -> multiplier 2\n",
    "\n",
    "    So:\n",
    "        C = base_C * (2 if input_rep == \"raw_log\" else 1)\n",
    "\n",
    "    Optional tunables in cfg:\n",
    "        cfg[\"use_dilation\"] = True / False\n",
    "        cfg[\"kernels\"]      = [3, 5, 5]      # keep final kernel smaller to reduce over-smoothing\n",
    "        cfg[\"dilations\"]    = [1, 2, 4]      # increasing dilation expands receptive field\n",
    "        cfg[\"channels\"]     = [32, 32, 16]   # out_channels per block\n",
    "        cfg[\"pool_k\"]       = 2              # MaxPool kernel\n",
    "        cfg[\"pool_s\"]       = 2              # MaxPool stride\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict, input_length: int = 3000, output_dim: int = 2):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # -----------------------------\n",
    "        # Infer input channels from cfg\n",
    "        # -----------------------------\n",
    "        base_C = {\"single\": 1, \"early_mid_late\": 3, \"hybrid_4ch\": 4}[cfg[\"channel_mode\"]]\n",
    "        mult = 2 if cfg.get(\"input_rep\", \"log\") == \"raw_log\" else 1\n",
    "        in_channels = base_C * mult\n",
    "\n",
    "        # -----------------------------\n",
    "        # Optional dilation settings\n",
    "        # -----------------------------\n",
    "        use_dilation = bool(cfg.get(\"use_dilation\", False))\n",
    "\n",
    "        # Kernels: increasing early->late but keep the final kernel modest\n",
    "        kernels = cfg.get(\"kernels\", [3, 5, 5])\n",
    "\n",
    "        # Dilations: increase only when use_dilation=True\n",
    "        dilations = cfg.get(\"dilations\", [1, 2, 4]) if use_dilation else [1, 1, 1]\n",
    "\n",
    "        # Out channels per conv block\n",
    "        chs = cfg.get(\"channels\", [32, 32, 16])\n",
    "\n",
    "        # Pooling\n",
    "        pool_k = int(cfg.get(\"pool_k\", 2))\n",
    "        pool_s = int(cfg.get(\"pool_s\", 2))\n",
    "\n",
    "        # Unpack \n",
    "        k1, k2, k3 = kernels\n",
    "        d1, d2, d3 = dilations\n",
    "\n",
    "        # -----------------------------\n",
    "        # Helper: SAME padding for 1D conv\n",
    "        # padding = dilation * (kernel - 1) / 2 (requires odd kernel)\n",
    "        # -----------------------------\n",
    "        def same_padding(kernel: int, dilation: int) -> int:\n",
    "            if kernel % 2 == 0:\n",
    "                raise ValueError(f\"Kernel size must be odd for SAME padding. Got kernel={kernel}.\")\n",
    "            return (dilation * (kernel - 1)) // 2\n",
    "\n",
    "        # -----------------------------\n",
    "        # Convolution blocks\n",
    "        # -----------------------------\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=chs[0],\n",
    "            kernel_size=k1,\n",
    "            dilation=d1,\n",
    "            padding=same_padding(k1, d1),\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(chs[0])\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=pool_k, stride=pool_s)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=chs[0],\n",
    "            out_channels=chs[1],\n",
    "            kernel_size=k2,\n",
    "            dilation=d2,\n",
    "            padding=same_padding(k2, d2),\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(chs[1])\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=pool_k, stride=pool_s)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=chs[1],\n",
    "            out_channels=chs[2],\n",
    "            kernel_size=k3,\n",
    "            dilation=d3,\n",
    "            padding=same_padding(k3, d3),\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(chs[2])\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=pool_k, stride=pool_s)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dynamic flatten dimension\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_length)\n",
    "            feat = self._forward_features(dummy)\n",
    "            self.flatten_dim = feat.shape[1]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fully connected head\n",
    "        # -----------------------------\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def _forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool1(self.act(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.act(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.act(self.bn3(self.conv3(x))))\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._forward_features(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a581ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    save_path=None,\n",
    "    eps=1e-12,\n",
    "    print_every=1,\n",
    "    exp_clip=20.0,    # exp(20) ~ 4.85e8, safety for overflow\n",
    "    patience=20,\n",
    "    min_delta=0.0,\n",
    "    plot_path=None,   # optional: where to save loss curve png\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a model that predicts log(mua), log(mus').\n",
    "    - Optimizes log-MSE loss\n",
    "    - Reports RMSE and mean absolute % error in original (linear) units\n",
    "    - Early stopping + optional best checkpoint saving\n",
    "    - Tracks and plots train/val loss curves over epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # -------------------------\n",
    "        # TRAIN\n",
    "        # -------------------------\n",
    "        model.train()\n",
    "\n",
    "        tr_loss_sum = 0.0\n",
    "        tr_loss_count = 0\n",
    "\n",
    "        tr_sum_sq  = torch.zeros(2, device=device)\n",
    "        tr_pct_sum = torch.zeros(2, device=device)\n",
    "        tr_count = 0\n",
    "\n",
    "        for signals, labels in train_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels  = labels.to(device).float()                 # (B,2) linear\n",
    "            B = labels.shape[0]\n",
    "\n",
    "            log_labels = torch.log(labels.clamp_min(eps))       # (B,2) log\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds_log = model(signals).view_as(log_labels)      # (B,2)\n",
    "            loss = loss_fn(preds_log, log_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # per-sample accurate averaging\n",
    "            tr_loss_sum += loss.item() * B\n",
    "            tr_loss_count += B\n",
    "\n",
    "            # Metrics in original units\n",
    "            with torch.no_grad():\n",
    "                preds_lin = torch.exp(preds_log.clamp(-exp_clip, exp_clip))\n",
    "                err = preds_lin - labels\n",
    "\n",
    "                abs_pct_err = (100.0 * err / labels.clamp_min(eps)).abs()  # (B,2)\n",
    "\n",
    "                tr_sum_sq  += (err ** 2).sum(dim=0)\n",
    "                tr_pct_sum += abs_pct_err.sum(dim=0)\n",
    "                tr_count   += B\n",
    "\n",
    "        train_loss = tr_loss_sum / max(1, tr_loss_count)\n",
    "        train_rmse = torch.sqrt(tr_sum_sq / max(1, tr_count)).detach().cpu().numpy()\n",
    "        train_pct_err = (tr_pct_sum / max(1, tr_count)).detach().cpu().numpy()\n",
    "\n",
    "        # -------------------------\n",
    "        # VALIDATE\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "\n",
    "        va_loss_sum = 0.0\n",
    "        va_loss_count = 0\n",
    "\n",
    "        va_sum_sq  = torch.zeros(2, device=device)\n",
    "        va_pct_sum = torch.zeros(2, device=device)\n",
    "        va_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in val_loader:\n",
    "                signals = signals.to(device)\n",
    "                labels  = labels.to(device).float()\n",
    "                B = labels.shape[0]\n",
    "\n",
    "                log_labels = torch.log(labels.clamp_min(eps))\n",
    "                preds_log  = model(signals).view_as(log_labels)\n",
    "\n",
    "                loss = loss_fn(preds_log, log_labels)\n",
    "                va_loss_sum += loss.item() * B\n",
    "                va_loss_count += B\n",
    "\n",
    "                preds_lin = torch.exp(preds_log.clamp(-exp_clip, exp_clip))\n",
    "                err = preds_lin - labels\n",
    "\n",
    "                abs_pct_err = (100.0 * err / labels.clamp_min(eps)).abs()\n",
    "\n",
    "                va_sum_sq  += (err ** 2).sum(dim=0)\n",
    "                va_pct_sum += abs_pct_err.sum(dim=0)\n",
    "                va_count   += B\n",
    "\n",
    "        val_loss = va_loss_sum / max(1, va_loss_count)\n",
    "        val_rmse = torch.sqrt(va_sum_sq / max(1, va_count)).detach().cpu().numpy()\n",
    "        val_pct_err = (va_pct_sum / max(1, va_count)).detach().cpu().numpy()\n",
    "\n",
    "        # Track loss curves\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Train Loss (log-MSE): {train_loss:.6f} | Train RMSE [μa, μs′]: {train_rmse}\")\n",
    "            print(f\"Train Mean Abs %Err [μa, μs′]: {train_pct_err}\")\n",
    "            print(f\"Val   Loss (log-MSE): {val_loss:.6f} | Val   RMSE [μa, μs′]: {val_rmse}\")\n",
    "            print(f\"Val   Mean Abs %Err [μa, μs′]: {val_pct_err}\")\n",
    "\n",
    "        # Early stopping + checkpoint\n",
    "        if val_loss < (best_val_loss - min_delta):\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            if save_path is not None:\n",
    "                os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\" -> Best validation so far, saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch + 1}: \"\n",
    "                      f\"no val improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # -------------------------\n",
    "    # Plot loss curves\n",
    "    # -------------------------\n",
    "    if plot_path is None and save_path is not None:\n",
    "        # If caller passed .pt or .pth, this handles both nicely\n",
    "        root, ext = os.path.splitext(save_path)\n",
    "        plot_path = root + \"_loss_curves.png\"\n",
    "\n",
    "    if plot_path is not None:\n",
    "        os.makedirs(os.path.dirname(plot_path) or \".\", exist_ok=True)\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses, label=\"Train log-MSE\")\n",
    "        plt.plot(val_losses,   label=\"Val log-MSE\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"save_path\": save_path,\n",
    "        \"loss_plot\": plot_path,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1bb93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_loaders(dataset, batch_size=32, val_frac=0.2, seed=42, shuffle_train=True):\n",
    "    n = len(dataset)\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    split = int(n * (1 - val_frac))\n",
    "    train_idx = idx[:split]\n",
    "    val_idx = idx[split:]\n",
    "\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2031f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_path = \"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/dataset_homo_small.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5483d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilation enabled: True\n",
      "kernels: [3, 5, 5] dilations: [1, 2, 4]\n",
      "sample: torch.Size([8, 3000]) torch.Size([2])\n",
      "model out: torch.Size([1, 2])\n",
      "signals: torch.Size([32, 8, 3000])\n",
      "labels: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "# Trial run 1: \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = {\n",
    "        \"crop_t_max\": 6.0,\n",
    "        \"sg_window\": frame_length,\n",
    "        \"sg_order\": order,\n",
    "        \"eps\": 1e-12,\n",
    "        \"channel_mode\": \"hybrid_4ch\",  # \"single\" | \"early_mid_late\" | \"hybrid_4ch\"\n",
    "        \"input_rep\": \"raw_log\",        # \"raw\" | \"log\" | \"raw_log\"\n",
    "        \"lr\": 10e-3,\n",
    "        # ---- OPTIONAL DILATION EXPERIMENT ----\n",
    "        \"use_dilation\": True,          # <-- set False for baseline comparison\n",
    "        \"kernels\": [3, 5, 5],          # increasing kernels but smaller final kernel\n",
    "        \"dilations\": [1, 2, 4],        # increasing dilation\n",
    "        \"channels\": [32, 32, 16],      # optional: keep your original channels\n",
    "        \"pool_k\": 2,\n",
    "        \"pool_s\": 2,\n",
    "    }\n",
    "\n",
    "    print(\"Dilation enabled:\", cfg[\"use_dilation\"])\n",
    "    if cfg[\"use_dilation\"]:\n",
    "        print(\"kernels:\", cfg[\"kernels\"], \"dilations:\", cfg[\"dilations\"])\n",
    "\n",
    "    # Dataset sanity check \n",
    "\n",
    "    ds = DTOFDataset(matlab_path, cfg)\n",
    "    x, y = ds[0]\n",
    "    print(\"sample:\", x.shape, y.shape)\n",
    "\n",
    "    # Model sanity check\n",
    "\n",
    "    model = Net(cfg, input_length=ds.T, output_dim=2).to(device)\n",
    "    x = x.to(device)\n",
    "    out = model(x.unsqueeze(0))\n",
    "    print(\"model out:\", out.shape)\n",
    "\n",
    "    # Loader sanity check \n",
    "    \n",
    "    train_loader, val_loader = make_train_val_loaders(ds, batch_size=32, val_frac=0.2, seed=42)\n",
    "\n",
    "    signals, labels = next(iter(train_loader))\n",
    "    print(\"signals:\", signals.shape)  # (B, C, T)\n",
    "    print(\"labels:\", labels.shape)    # (B, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "414ed861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator: \n",
    "    \"\"\"\n",
    "    Evaluates a trained model that outputs log(mua), log(mus).\n",
    "    Reports MAE/RMSE in original units by exp().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device, eps=1e-12):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.eps = eps\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        all_preds_lin = []\n",
    "        all_labels_lin = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in data_loader:\n",
    "                signals = signals.to(self.device)\n",
    "                labels = labels.to(self.device).float()              # (B,2) linear\n",
    "\n",
    "                preds_log = self.model(signals)                      # (B,2) log\n",
    "                preds_log = preds_log.view_as(labels)\n",
    "\n",
    "                preds_lin = torch.exp(preds_log)                     # back to linear\n",
    "\n",
    "                all_preds_lin.append(preds_lin.cpu())\n",
    "                all_labels_lin.append(labels.cpu())\n",
    "\n",
    "        preds = torch.cat(all_preds_lin, dim=0)\n",
    "        labs  = torch.cat(all_labels_lin, dim=0)\n",
    "\n",
    "        abs_err = torch.abs(preds - labs)\n",
    "        sq_err  = (preds - labs) ** 2\n",
    "\n",
    "        mae = abs_err.mean(dim=0)\n",
    "        rmse = torch.sqrt(sq_err.mean(dim=0))\n",
    "\n",
    "        return {\n",
    "            \"MAE\": mae.numpy(),\n",
    "            \"RMSE\": rmse.numpy(),\n",
    "            \"preds\": preds.numpy(),\n",
    "            \"labels\": labs.numpy(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f15bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total samples: 500\n",
      "Train samples: 400\n",
      "Val samples  : 100\n",
      "Signal shape : torch.Size([8, 3000]) Label shape: torch.Size([2])\n",
      "steps_per_epoch: 13\n",
      "target_steps   : 50000\n",
      "num_epochs     : 200\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# RUN SCRIPT (baseline vs dilated) + auto epoch scheduling + early stopping\n",
    "# ============================\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def make_train_val_loaders(dataset, batch_size=32, val_frac=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    split = int(n * (1 - val_frac))\n",
    "    train_idx = idx[:split]\n",
    "    val_idx = idx[split:]\n",
    "\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def quick_eval_lin_rmse(model, loader, device, eps=1e-12, exp_clip=20.0):\n",
    "    \"\"\"Compute RMSE in original units (model outputs log-space).\"\"\"\n",
    "    model.eval()\n",
    "    sum_sq = torch.zeros(2, device=device)\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).float()\n",
    "            ylog = torch.log(y.clamp_min(eps))\n",
    "            plog = model(x).view_as(ylog)\n",
    "            plin = torch.exp(plog.clamp(-exp_clip, exp_clip))\n",
    "            err = plin - y\n",
    "            sum_sq += (err ** 2).sum(dim=0)\n",
    "            n += y.shape[0]\n",
    "    return torch.sqrt(sum_sq / max(1, n)).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Paths + base config\n",
    "# ----------------------------\n",
    "mat_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/dataset_homo_small.mat\"\n",
    "\n",
    "base_cfg = {\n",
    "    \"crop_t_max\": 6.0,\n",
    "    \"sg_window\": 31,\n",
    "    \"sg_order\": 1,\n",
    "    \"eps\": 1e-12,\n",
    "    \"channel_mode\": \"hybrid_4ch\",\n",
    "    \"input_rep\": \"raw_log\",\n",
    "    \"lr\": 1e-3,\n",
    "\n",
    "    # ---- DILATION EXPERIMENT DEFAULTS (OFF unless enabled below) ----\n",
    "    \"use_dilation\": False,\n",
    "    \"kernels\": [3, 5, 5],      # increasing kernels, smaller final kernel\n",
    "    \"dilations\": [1, 2, 4],    # increasing dilation (only used if use_dilation=True)\n",
    "    \"channels\": [32, 32, 16],\n",
    "    \"pool_k\": 2,\n",
    "    \"pool_s\": 2,\n",
    "\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Device\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset + split (DO THIS ONCE so both models share identical split)\n",
    "# ----------------------------\n",
    "dataset = DTOFDataset(mat_path, base_cfg)\n",
    "\n",
    "batch_size = 32\n",
    "val_frac = 0.2\n",
    "seed = 42\n",
    "\n",
    "train_loader, val_loader = make_train_val_loaders(dataset, batch_size=batch_size, val_frac=val_frac, seed=seed)\n",
    "\n",
    "N_total = len(dataset)\n",
    "N_train = len(train_loader.dataset)\n",
    "N_val = len(val_loader.dataset)\n",
    "\n",
    "print(\"Total samples:\", N_total)\n",
    "print(\"Train samples:\", N_train)\n",
    "print(\"Val samples  :\", N_val)\n",
    "print(\"Signal shape :\", dataset[0][0].shape, \"Label shape:\", dataset[0][1].shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Epoch scheduling (scaled by dataset size via target optimiser steps)\n",
    "# ----------------------------\n",
    "target_steps = 50_000  # adjust later\n",
    "steps_per_epoch = math.ceil(N_train / batch_size)\n",
    "num_epochs = 200\n",
    "\n",
    "\n",
    "print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "print(f\"target_steps   : {target_steps}\")\n",
    "print(f\"num_epochs     : {num_epochs}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Run function (so we can compare baseline vs dilated cleanly)\n",
    "# ----------------------------\n",
    "def run_experiment(cfg, tag):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"EXPERIMENT: {tag}\")\n",
    "    print(f\"use_dilation={cfg.get('use_dilation', False)} | kernels={cfg.get('kernels')} | dilations={cfg.get('dilations')}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model = Net(cfg, input_length=dataset.T, output_dim=2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "\n",
    "    # Pre-train sanity RMSE\n",
    "    rmse0_train = quick_eval_lin_rmse(model, train_loader, device, eps=cfg[\"eps\"])\n",
    "    rmse0_val = quick_eval_lin_rmse(model, val_loader, device, eps=cfg[\"eps\"])\n",
    "    print(\"Pre-train RMSE train [mua, mus]:\", rmse0_train)\n",
    "    print(\"Pre-train RMSE val   [mua, mus]:\", rmse0_val)\n",
    "\n",
    "    save_path = f\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/CNN_initial_saved_pytorch_model_weights/best_{tag}.pt\"\n",
    "\n",
    "    plot_path = save_path.replace(\".pt\", \"_loss_curves.png\")\n",
    "\n",
    "    train_out = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        save_path=save_path,\n",
    "        plot_path=plot_path,     \n",
    "        eps=cfg[\"eps\"],\n",
    "        print_every=1,\n",
    "        patience=20,\n",
    "        min_delta=1e-4,\n",
    "    )\n",
    "\n",
    "    print(\"Saved loss curves to:\", train_out[\"loss_plot\"])\n",
    "\n",
    "    # Post-train RMSE\n",
    "    rmse1_train = quick_eval_lin_rmse(model, train_loader, device, eps=cfg[\"eps\"])\n",
    "    rmse1_val = quick_eval_lin_rmse(model, val_loader, device, eps=cfg[\"eps\"])\n",
    "    print(\"Post-train RMSE train [mua, mus]:\", rmse1_train)\n",
    "    print(\"Post-train RMSE val   [mua, mus]:\", rmse1_val)\n",
    "\n",
    "    # One-batch forward shape check\n",
    "    signals, labels = next(iter(train_loader))\n",
    "    out = model(signals.to(device))\n",
    "    print(\"One-batch shapes:\")\n",
    "    print(\"  signals:\", signals.shape)\n",
    "    print(\"  labels :\", labels.shape)\n",
    "    print(\"  output :\", out.shape)\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"rmse_train\": rmse1_train,\n",
    "        \"rmse_val\": rmse1_val,\n",
    "        \"save_path\": save_path,\n",
    "        \"loss_plot\": train_out[\"loss_plot\"],\n",
    "        \"cfg\": cfg, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT: E1A_baseline_k333_d111\n",
      "use_dilation=False | kernels=[3, 3, 3] | dilations=[1, 2, 4]\n",
      "======================================================================\n",
      "Pre-train RMSE train [mua, mus]: [ 1.1892968 13.672103 ]\n",
      "Pre-train RMSE val   [mua, mus]: [ 1.2100372 13.595335 ]\n",
      "\n",
      "Epoch 1/200\n",
      "Train Loss (log-MSE): 3.061539 | Train RMSE [μa, μs′]: [ 0.37179145 12.001267  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [1601.0021   133.92024]\n",
      "Val   Loss (log-MSE): 4.072364 | Val   RMSE [μa, μs′]: [ 0.201148 12.143765]\n",
      "Val   Mean Abs %Err [μa, μs′]: [1400.8514    58.25841]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 2/200\n",
      "Train Loss (log-MSE): 0.451710 | Train RMSE [μa, μs′]: [ 0.01653597 11.895226  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [51.577904 67.284454]\n",
      "Val   Loss (log-MSE): 0.526896 | Val   RMSE [μa, μs′]: [0.02573122 8.222477  ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [138.35645  34.84468]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 3/200\n",
      "Train Loss (log-MSE): 0.164345 | Train RMSE [μa, μs′]: [0.01263787 7.251686  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [33.172253 33.74794 ]\n",
      "Val   Loss (log-MSE): 0.313631 | Val   RMSE [μa, μs′]: [0.02265742 5.9585423 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [93.1822  32.59788]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 4/200\n",
      "Train Loss (log-MSE): 0.226476 | Train RMSE [μa, μs′]: [0.02297089 4.2778506 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [60.43751  29.749584]\n",
      "Val   Loss (log-MSE): 0.058007 | Val   RMSE [μa, μs′]: [0.00583488 3.3674042 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [25.045713 18.470766]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 5/200\n",
      "Train Loss (log-MSE): 0.093576 | Train RMSE [μa, μs′]: [0.01133947 3.2344363 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [28.411516 19.152756]\n",
      "Val   Loss (log-MSE): 0.068569 | Val   RMSE [μa, μs′]: [0.01032623 3.2146444 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [35.259445 13.996452]\n",
      "\n",
      "Epoch 6/200\n",
      "Train Loss (log-MSE): 0.086294 | Train RMSE [μa, μs′]: [0.01343696 3.1255224 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [29.830961 16.378485]\n",
      "Val   Loss (log-MSE): 0.017356 | Val   RMSE [μa, μs′]: [0.00458148 2.6085525 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [ 9.058462 12.469563]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 7/200\n",
      "Train Loss (log-MSE): 0.102314 | Train RMSE [μa, μs′]: [0.01168763 3.5347636 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [31.53158  17.525969]\n",
      "Val   Loss (log-MSE): 0.018491 | Val   RMSE [μa, μs′]: [0.00514791 2.5781422 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [ 8.874316 13.012704]\n",
      "\n",
      "Epoch 8/200\n",
      "Train Loss (log-MSE): 0.063786 | Train RMSE [μa, μs′]: [0.00905443 3.56743   ]\n",
      "Train Mean Abs %Err [μa, μs′]: [24.83937  16.947668]\n",
      "Val   Loss (log-MSE): 0.025328 | Val   RMSE [μa, μs′]: [0.00353392 3.1152442 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [ 9.905767 15.644658]\n",
      "\n",
      "Epoch 9/200\n",
      "Train Loss (log-MSE): 0.053505 | Train RMSE [μa, μs′]: [0.00960596 3.189208  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [21.29447  15.584036]\n",
      "Val   Loss (log-MSE): 0.084402 | Val   RMSE [μa, μs′]: [0.00885644 2.4325893 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [26.26593  13.033085]\n",
      "\n",
      "Epoch 10/200\n",
      "Train Loss (log-MSE): 0.041466 | Train RMSE [μa, μs′]: [0.00997724 2.9286482 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [18.477594 14.202808]\n",
      "Val   Loss (log-MSE): 0.014565 | Val   RMSE [μa, μs′]: [0.00429166 1.6924993 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [11.104905  7.759223]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 11/200\n",
      "Train Loss (log-MSE): 0.038022 | Train RMSE [μa, μs′]: [0.01096982 2.1783626 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [17.786232 11.195584]\n",
      "Val   Loss (log-MSE): 0.018845 | Val   RMSE [μa, μs′]: [0.00602765 2.5320418 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [11.1809225 10.84124  ]\n",
      "\n",
      "Epoch 12/200\n",
      "Train Loss (log-MSE): 0.073138 | Train RMSE [μa, μs′]: [0.00750138 2.0607207 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [30.48946   11.9832945]\n",
      "Val   Loss (log-MSE): 0.020778 | Val   RMSE [μa, μs′]: [0.00347363 1.5288987 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [16.576807  9.546183]\n",
      "\n",
      "Epoch 13/200\n",
      "Train Loss (log-MSE): 0.035923 | Train RMSE [μa, μs′]: [0.00577162 1.9777954 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [16.759272  9.846541]\n",
      "Val   Loss (log-MSE): 0.012137 | Val   RMSE [μa, μs′]: [0.00249636 1.9680147 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [7.2258043 8.035169 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 14/200\n",
      "Train Loss (log-MSE): 0.024084 | Train RMSE [μa, μs′]: [0.0064305 1.6074858]\n",
      "Train Mean Abs %Err [μa, μs′]: [15.069497  9.283901]\n",
      "Val   Loss (log-MSE): 0.009625 | Val   RMSE [μa, μs′]: [0.00675299 1.4800003 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [9.464422 6.430743]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 15/200\n",
      "Train Loss (log-MSE): 0.036813 | Train RMSE [μa, μs′]: [0.00780655 2.0402088 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [19.591461 10.658689]\n",
      "Val   Loss (log-MSE): 0.032935 | Val   RMSE [μa, μs′]: [0.00854218 1.3466456 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [21.031334   6.0780926]\n",
      "\n",
      "Epoch 16/200\n",
      "Train Loss (log-MSE): 0.032254 | Train RMSE [μa, μs′]: [0.00799731 2.487878  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [17.33965  11.619555]\n",
      "Val   Loss (log-MSE): 0.027513 | Val   RMSE [μa, μs′]: [0.00640715 1.1434087 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [18.253029   7.8846397]\n",
      "\n",
      "Epoch 17/200\n",
      "Train Loss (log-MSE): 0.037676 | Train RMSE [μa, μs′]: [0.00814186 2.3571737 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [18.721502 13.08247 ]\n",
      "Val   Loss (log-MSE): 0.033201 | Val   RMSE [μa, μs′]: [0.01127646 1.8209746 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [19.006763  7.4934  ]\n",
      "\n",
      "Epoch 18/200\n",
      "Train Loss (log-MSE): 0.034345 | Train RMSE [μa, μs′]: [0.01020781 2.1322732 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [19.439991 10.438782]\n",
      "Val   Loss (log-MSE): 0.108302 | Val   RMSE [μa, μs′]: [0.01372633 1.7545396 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [35.12601  11.101088]\n",
      "\n",
      "Epoch 19/200\n",
      "Train Loss (log-MSE): 0.027928 | Train RMSE [μa, μs′]: [0.0059103 1.7504658]\n",
      "Train Mean Abs %Err [μa, μs′]: [16.315378  9.011374]\n",
      "Val   Loss (log-MSE): 0.011448 | Val   RMSE [μa, μs′]: [0.00694771 1.8309423 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [11.414804   6.3242598]\n",
      "\n",
      "Epoch 20/200\n",
      "Train Loss (log-MSE): 0.036980 | Train RMSE [μa, μs′]: [0.0090994 1.4933591]\n",
      "Train Mean Abs %Err [μa, μs′]: [22.226723  8.131849]\n",
      "Val   Loss (log-MSE): 0.020832 | Val   RMSE [μa, μs′]: [0.00878259 1.5107845 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [15.577998  8.142148]\n",
      "\n",
      "Epoch 21/200\n",
      "Train Loss (log-MSE): 0.031492 | Train RMSE [μa, μs′]: [0.00835848 2.0001352 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [17.125439  9.993769]\n",
      "Val   Loss (log-MSE): 0.025083 | Val   RMSE [μa, μs′]: [0.00586584 1.8464682 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [13.757364 12.176963]\n",
      "\n",
      "Epoch 22/200\n",
      "Train Loss (log-MSE): 0.018664 | Train RMSE [μa, μs′]: [0.00618913 2.0348005 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [12.554957 10.045129]\n",
      "Val   Loss (log-MSE): 0.013451 | Val   RMSE [μa, μs′]: [0.00292183 2.2314913 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [8.392389 8.548243]\n",
      "\n",
      "Epoch 23/200\n",
      "Train Loss (log-MSE): 0.038457 | Train RMSE [μa, μs′]: [0.00797479 2.7616098 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [17.237144 15.406153]\n",
      "Val   Loss (log-MSE): 0.023882 | Val   RMSE [μa, μs′]: [0.00531663 3.027527  ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [11.778873 15.303989]\n",
      "\n",
      "Epoch 24/200\n",
      "Train Loss (log-MSE): 0.028954 | Train RMSE [μa, μs′]: [0.0076148 2.1805735]\n",
      "Train Mean Abs %Err [μa, μs′]: [16.32065  10.723037]\n",
      "Val   Loss (log-MSE): 0.009206 | Val   RMSE [μa, μs′]: [0.00165054 1.4729813 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [7.0532265 6.6071606]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 25/200\n",
      "Train Loss (log-MSE): 0.024041 | Train RMSE [μa, μs′]: [0.00418486 1.9648836 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [15.600029  8.474037]\n",
      "Val   Loss (log-MSE): 0.008165 | Val   RMSE [μa, μs′]: [0.00212965 0.86536384]\n",
      "Val   Mean Abs %Err [μa, μs′]: [9.386355  4.9268737]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 26/200\n",
      "Train Loss (log-MSE): 0.020741 | Train RMSE [μa, μs′]: [0.00541045 2.0182714 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [12.987202  9.489889]\n",
      "Val   Loss (log-MSE): 0.008841 | Val   RMSE [μa, μs′]: [2.0701438e-03 2.3434823e+00]\n",
      "Val   Mean Abs %Err [μa, μs′]: [4.300869  8.5311985]\n",
      "\n",
      "Epoch 27/200\n",
      "Train Loss (log-MSE): 0.053622 | Train RMSE [μa, μs′]: [0.00662949 2.8878303 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [21.415094 13.161238]\n",
      "Val   Loss (log-MSE): 0.030378 | Val   RMSE [μa, μs′]: [0.0037987 2.4078414]\n",
      "Val   Mean Abs %Err [μa, μs′]: [16.20808   9.246295]\n",
      "\n",
      "Epoch 28/200\n",
      "Train Loss (log-MSE): 0.042925 | Train RMSE [μa, μs′]: [0.00943357 3.2737863 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [19.664118 13.258423]\n",
      "Val   Loss (log-MSE): 0.034592 | Val   RMSE [μa, μs′]: [0.00469708 0.90287143]\n",
      "Val   Mean Abs %Err [μa, μs′]: [25.8567    5.439503]\n",
      "\n",
      "Epoch 29/200\n",
      "Train Loss (log-MSE): 0.037535 | Train RMSE [μa, μs′]: [0.00668253 2.2611856 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [20.825132  9.13578 ]\n",
      "Val   Loss (log-MSE): 0.056738 | Val   RMSE [μa, μs′]: [0.00644128 2.9069686 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [23.065437 15.799747]\n",
      "\n",
      "Epoch 30/200\n",
      "Train Loss (log-MSE): 0.047914 | Train RMSE [μa, μs′]: [0.00569252 3.1773503 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [19.158066 15.454124]\n",
      "Val   Loss (log-MSE): 0.018855 | Val   RMSE [μa, μs′]: [1.8583895e-03 3.6793125e+00]\n",
      "Val   Mean Abs %Err [μa, μs′]: [ 3.7524579 14.348532 ]\n",
      "\n",
      "Epoch 31/200\n",
      "Train Loss (log-MSE): 0.024446 | Train RMSE [μa, μs′]: [0.00486701 3.1771488 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [10.010463 14.818084]\n",
      "Val   Loss (log-MSE): 0.021256 | Val   RMSE [μa, μs′]: [0.00274666 1.7617077 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [10.066524 15.492415]\n",
      "\n",
      "Epoch 32/200\n",
      "Train Loss (log-MSE): 0.025231 | Train RMSE [μa, μs′]: [0.00494558 2.4395301 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [11.926797 12.582092]\n",
      "Val   Loss (log-MSE): 0.022343 | Val   RMSE [μa, μs′]: [1.8427555e-03 3.4246187e+00]\n",
      "Val   Mean Abs %Err [μa, μs′]: [ 5.5074162 15.794028 ]\n",
      "\n",
      "Epoch 33/200\n",
      "Train Loss (log-MSE): 0.021817 | Train RMSE [μa, μs′]: [0.00590569 2.0248768 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [11.26073   11.1043005]\n",
      "Val   Loss (log-MSE): 0.013809 | Val   RMSE [μa, μs′]: [0.00570245 1.2376723 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [13.189287   5.5869527]\n",
      "\n",
      "Epoch 34/200\n",
      "Train Loss (log-MSE): 0.023769 | Train RMSE [μa, μs′]: [0.00570782 2.411658  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [14.091493 11.22849 ]\n",
      "Val   Loss (log-MSE): 0.006232 | Val   RMSE [μa, μs′]: [0.00255676 0.8510293 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [7.604126  4.6212573]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 35/200\n",
      "Train Loss (log-MSE): 0.023324 | Train RMSE [μa, μs′]: [0.00749731 2.551085  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [13.576447 11.765006]\n",
      "Val   Loss (log-MSE): 0.009557 | Val   RMSE [μa, μs′]: [0.00315138 1.7841991 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [8.116173 8.096989]\n",
      "\n",
      "Epoch 36/200\n",
      "Train Loss (log-MSE): 0.017615 | Train RMSE [μa, μs′]: [0.00685127 2.092858  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [11.157224 10.139424]\n",
      "Val   Loss (log-MSE): 0.007752 | Val   RMSE [μa, μs′]: [0.00546533 1.402646  ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [7.9884243 7.0369573]\n",
      "\n",
      "Epoch 37/200\n",
      "Train Loss (log-MSE): 0.034987 | Train RMSE [μa, μs′]: [0.00910448 3.2647622 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [14.573818 14.368048]\n",
      "Val   Loss (log-MSE): 0.015862 | Val   RMSE [μa, μs′]: [0.00663278 2.4970987 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [10.857358  9.244584]\n",
      "\n",
      "Epoch 38/200\n",
      "Train Loss (log-MSE): 0.027521 | Train RMSE [μa, μs′]: [0.00800977 2.384112  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [14.549172 12.103032]\n",
      "Val   Loss (log-MSE): 0.012247 | Val   RMSE [μa, μs′]: [0.00227146 1.2592503 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [9.889979 8.199628]\n",
      "\n",
      "Epoch 39/200\n",
      "Train Loss (log-MSE): 0.014077 | Train RMSE [μa, μs′]: [0.00515509 1.9857305 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [9.8259535 9.583979 ]\n",
      "Val   Loss (log-MSE): 0.007828 | Val   RMSE [μa, μs′]: [0.00187534 1.7255557 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [6.5995693 5.909827 ]\n",
      "\n",
      "Epoch 40/200\n",
      "Train Loss (log-MSE): 0.010878 | Train RMSE [μa, μs′]: [0.00421568 1.8616419 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [7.5531797 8.40431  ]\n",
      "Val   Loss (log-MSE): 0.012472 | Val   RMSE [μa, μs′]: [0.00434379 1.2385063 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [13.888866  5.364371]\n",
      "\n",
      "Epoch 41/200\n",
      "Train Loss (log-MSE): 0.028871 | Train RMSE [μa, μs′]: [0.0065814 2.2771933]\n",
      "Train Mean Abs %Err [μa, μs′]: [17.281624  9.933566]\n",
      "Val   Loss (log-MSE): 0.008483 | Val   RMSE [μa, μs′]: [0.00290041 1.0059485 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [9.064895 5.81008 ]\n",
      "\n",
      "Epoch 42/200\n",
      "Train Loss (log-MSE): 0.026233 | Train RMSE [μa, μs′]: [0.00605252 2.664719  ]\n",
      "Train Mean Abs %Err [μa, μs′]: [14.701299 12.079273]\n",
      "Val   Loss (log-MSE): 0.009148 | Val   RMSE [μa, μs′]: [0.00207464 1.3691097 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [7.3596153 8.78946  ]\n",
      "\n",
      "Epoch 43/200\n",
      "Train Loss (log-MSE): 0.018548 | Train RMSE [μa, μs′]: [0.0034068 2.571752 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [ 9.8793545 11.86094  ]\n",
      "Val   Loss (log-MSE): 0.015861 | Val   RMSE [μa, μs′]: [0.00354625 2.6282022 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [10.374543 11.81335 ]\n",
      "\n",
      "Epoch 44/200\n",
      "Train Loss (log-MSE): 0.020764 | Train RMSE [μa, μs′]: [0.00452398 1.6437918 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [13.664745  8.757225]\n",
      "Val   Loss (log-MSE): 0.003809 | Val   RMSE [μa, μs′]: [0.00136281 1.1750487 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [5.0117083 4.4111247]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 45/200\n",
      "Train Loss (log-MSE): 0.016164 | Train RMSE [μa, μs′]: [0.00543331 1.7877462 ]\n",
      "Train Mean Abs %Err [μa, μs′]: [13.282102   6.8423443]\n",
      "Val   Loss (log-MSE): 0.010961 | Val   RMSE [μa, μs′]: [0.00401627 1.3141168 ]\n",
      "Val   Mean Abs %Err [μa, μs′]: [13.173473   4.6887507]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# EXPERIMENT 1: Dilation as a substitute for depth (fixed small kernels)\n",
    "# Question: With a shallow 3-layer CNN, how much performance do we gain purely\n",
    "#           from increasing receptive field via dilation?\n",
    "# Keep kernels fixed to isolate dilation as the mechanism.\n",
    "# ============================\n",
    "\n",
    "# 1A: baseline, no dilation\n",
    "cfg_e1a = dict(base_cfg)\n",
    "cfg_e1a[\"kernels\"] = [3, 3, 3]\n",
    "cfg_e1a[\"use_dilation\"] = False\n",
    "\n",
    "# 1B: moderate dilation\n",
    "cfg_e1b = dict(base_cfg)\n",
    "cfg_e1b[\"kernels\"] = [3, 3, 3]\n",
    "cfg_e1b[\"use_dilation\"] = True\n",
    "cfg_e1b[\"dilations\"] = [1, 2, 4]\n",
    "\n",
    "# 1C: stronger dilation\n",
    "cfg_e1c = dict(base_cfg)\n",
    "cfg_e1c[\"kernels\"] = [3, 3, 3]\n",
    "cfg_e1c[\"use_dilation\"] = True\n",
    "cfg_e1c[\"dilations\"] = [1, 4, 8]\n",
    "\n",
    "res_e1a = run_experiment(cfg_e1a, tag=\"E1A_baseline_k333_d111\")\n",
    "res_e1b = run_experiment(cfg_e1b, tag=\"E1B_dilated_k333_d124\")\n",
    "res_e1c = run_experiment(cfg_e1c, tag=\"E1C_dilated_k333_d148\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# EXPERIMENT 2: Kernel size vs dilation at constant depth\n",
    "# Question: If we keep the depth at 3 layers, is it better to enlarge receptive\n",
    "#           fields using bigger kernels or dilation?\n",
    "# ============================\n",
    "\n",
    "# 2A: big kernels, no dilation\n",
    "cfg_e2a = dict(base_cfg)\n",
    "cfg_e2a[\"kernels\"] = [11, 11, 11]\n",
    "cfg_e2a[\"use_dilation\"] = False\n",
    "\n",
    "# 2B: small kernels + dilation\n",
    "cfg_e2b = dict(base_cfg)\n",
    "cfg_e2b[\"kernels\"] = [3, 3, 3]\n",
    "cfg_e2b[\"use_dilation\"] = True\n",
    "cfg_e2b[\"dilations\"] = [1, 2, 4]\n",
    "\n",
    "res_e2a = run_experiment(cfg_e2a, tag=\"E2A_bigkern_k111111_d111\")\n",
    "res_e2b = run_experiment(cfg_e2b, tag=\"E2B_dilated_k333_d124\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# EXPERIMENT 3: Temporal multi-scale allocation in a shallow network\n",
    "# Question: With depth fixed, does performance improve when the shallow CNN\n",
    "#           allocates temporal resolution differently across layers?\n",
    "# ============================\n",
    "\n",
    "# 3A: increasing kernels\n",
    "cfg_e3a = dict(base_cfg)\n",
    "cfg_e3a[\"kernels\"] = [3, 5, 7]\n",
    "cfg_e3a[\"use_dilation\"] = True\n",
    "cfg_e3a[\"dilations\"] = [1, 2, 4]\n",
    "\n",
    "# 3B: decreasing kernels\n",
    "cfg_e3b = dict(base_cfg)\n",
    "cfg_e3b[\"kernels\"] = [7, 5, 3]\n",
    "cfg_e3b[\"use_dilation\"] = True\n",
    "cfg_e3b[\"dilations\"] = [1, 2, 4]\n",
    "\n",
    "res_e3a = run_experiment(cfg_e3a, tag=\"E3A_inc_k357_d124\")\n",
    "res_e3b = run_experiment(cfg_e3b, tag=\"E3B_dec_k753_d124\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SUMMARY\n",
    "# ============================\n",
    "all_results = [res_e1a, res_e1b, res_e1c, res_e2a, res_e2b, res_e3a, res_e3b]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY (Post-train RMSE on validation set)\")\n",
    "print(\"=\" * 70)\n",
    "for r in all_results:\n",
    "    print(f\"{r['tag']}: val_RMSE=[μa={r['rmse_val'][0]:.6f}, μs′={r['rmse_val'][1]:.6f}]  | loss_plot={r['loss_plot']}\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
