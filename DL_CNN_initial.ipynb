{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b73156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import random \n",
    "import torch \n",
    "from torch import nn \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader,Subset\n",
    "from scipy.signal import savgol_filter \n",
    "import h5py\n",
    "random.seed(0)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Savitzky Golay filter parameters \n",
    "order = 1\n",
    "frame_length = 21\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b40d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b358256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTOFDataset(Dataset):\n",
    "    \"\"\"\n",
    "        DTOF dataset for MATLAB v7.3 (.mat, HDF5) files.\n",
    "\n",
    "        Required datasets inside .mat:\n",
    "            X : (Nt, N) or (N, Nt)  reflectance DTOFs\n",
    "            y : (2, N)  or (N, 2)   [mua, mus']\n",
    "            t : (Nt,)              time vector in seconds (~1e-12 resolution)\n",
    "\n",
    "        Preprocessing:\n",
    "            - convert t from seconds -> ns\n",
    "            - crop [0, crop_t_max] ns\n",
    "            - Savitzky–Golay smoothing\n",
    "            - clip small values to eps\n",
    "            - input representation:\n",
    "                * \"raw\"      -> use reflectance (smoothed+clipped)\n",
    "                * \"log\"      -> use log(reflectance)\n",
    "                * \"raw_log\"  -> concatenate raw and log along channel dimension\n",
    "            - channel construction:\n",
    "                * \"single\"         -> 1 channel (full)\n",
    "                * \"early_mid_late\" -> 3 channels (early/mid/late masks)\n",
    "                * \"hybrid_4ch\"     -> 4 channels (full + early/mid/late masks)\n",
    "\n",
    "        Returns:\n",
    "            signal: (C, T) float32 tensor\n",
    "            label:  (2,) float32 tensor [mua, mus']  (raw labels for now)\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, mat_path: str, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # ---------- load HDF5 (.mat v7.3) ----------\n",
    "        with h5py.File(mat_path, \"r\") as f:\n",
    "            X = np.array(f[\"X\"], dtype=np.float32)\n",
    "            y = np.array(f[\"y\"], dtype=np.float32)\n",
    "            t = np.array(f[\"t\"], dtype=np.float32).squeeze()\n",
    "\n",
    "        # ---------- normalise shapes ----------\n",
    "        # X -> (N, Nt)\n",
    "        if X.shape[0] == t.shape[0]:\n",
    "            X = X.T\n",
    "\n",
    "        # y -> (N, 2)\n",
    "        if y.shape[0] == 2:\n",
    "            y = y.T\n",
    "\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(f\"Expected X to be 2D, got {X.shape}\")\n",
    "        if y.ndim != 2 or y.shape[1] != 2:\n",
    "            raise ValueError(f\"Expected y to be (N,2), got {y.shape}\")\n",
    "        if t.ndim != 1:\n",
    "            raise ValueError(f\"Expected t to be (Nt,), got {t.shape}\")\n",
    "        if X.shape[1] != t.shape[0]:\n",
    "            raise ValueError(f\"X and t mismatch: X Nt={X.shape[1]} vs t Nt={t.shape[0]}\")\n",
    "\n",
    "        # ---------- time: seconds -> ns ----------\n",
    "        t_ns = t * 1e9\n",
    "\n",
    "        # ---------- crop ----------\n",
    "        crop_t_max = float(cfg[\"crop_t_max\"])  # ns\n",
    "        t_mask = (t_ns >= 0.0) & (t_ns <= crop_t_max)\n",
    "        if not np.any(t_mask):\n",
    "            raise ValueError(\n",
    "                f\"Cropping removed all points. \"\n",
    "                f\"t_ns range=[{t_ns.min():.3g}, {t_ns.max():.3g}] ns, crop_t_max={crop_t_max}\"\n",
    "            )\n",
    "\n",
    "        t_ns = t_ns[t_mask]              # (T,)\n",
    "        dtof = X[:, t_mask]              # (N,T)\n",
    "\n",
    "        N, T = dtof.shape\n",
    "\n",
    "        # ---------- Savitzky–Golay ----------\n",
    "        sg_window = int(cfg[\"sg_window\"])\n",
    "        sg_order = int(cfg[\"sg_order\"])\n",
    "\n",
    "        # enforce validity (odd, > order, <= T)\n",
    "        if sg_window % 2 == 0:\n",
    "            sg_window += 1\n",
    "        if sg_window <= sg_order:\n",
    "            sg_window = sg_order + 2\n",
    "            if sg_window % 2 == 0:\n",
    "                sg_window += 1\n",
    "        if sg_window > T:\n",
    "            sg_window = T if (T % 2 == 1) else (T - 1)\n",
    "\n",
    "        if sg_window >= 3:\n",
    "            dtof = savgol_filter(dtof, sg_window, sg_order, axis=1)\n",
    "\n",
    "        # ---------- clip ----------\n",
    "        eps = float(cfg.get(\"eps\", 1e-12))\n",
    "        dtof[dtof < eps] = eps\n",
    "\n",
    "        # ---------- choose representation ----------\n",
    "        input_rep = cfg.get(\"input_rep\", \"log\")  # \"raw\" | \"log\" | \"raw_log\"\n",
    "\n",
    "        dtof_raw = dtof.astype(np.float32)\n",
    "        dtof_log = np.log(dtof_raw).astype(np.float32)\n",
    "\n",
    "        # ---------- build channels ----------\n",
    "        if input_rep == \"raw\":\n",
    "            channels = self.build_channels(t_ns, dtof_raw, cfg[\"channel_mode\"])  # (N,C,T)\n",
    "\n",
    "        elif input_rep == \"log\":\n",
    "            channels = self.build_channels(t_ns, dtof_log, cfg[\"channel_mode\"])  # (N,C,T)\n",
    "\n",
    "        elif input_rep == \"raw_log\":\n",
    "            ch_raw = self.build_channels(t_ns, dtof_raw, cfg[\"channel_mode\"])    # (N,C,T)\n",
    "            ch_log = self.build_channels(t_ns, dtof_log, cfg[\"channel_mode\"])    # (N,C,T)\n",
    "            channels = np.concatenate([ch_raw, ch_log], axis=1)                  # (N,2C,T)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input_rep: {input_rep}\")\n",
    "\n",
    "        # ---------- to torch ----------\n",
    "        self.signals = torch.tensor(channels, dtype=torch.float32)  # (N,C,T)\n",
    "        self.labels = torch.tensor(y, dtype=torch.float32)          # (N,2)\n",
    "\n",
    "        self.N, self.C, self.T = self.signals.shape\n",
    "\n",
    "    def build_channels(self, t_ns: np.ndarray, dtof: np.ndarray, mode: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Channel masks in ns:\n",
    "            early: 0–0.5 ns\n",
    "            mid:   0.5–4 ns\n",
    "            late:  4–crop_t_max ns\n",
    "        \"\"\"\n",
    "        N, T = dtof.shape\n",
    "        crop_t_max = float(self.cfg[\"crop_t_max\"])\n",
    "\n",
    "        if mode == \"single\":\n",
    "            return dtof[:, None, :]  # (N,1,T)\n",
    "\n",
    "        early = ((t_ns >= 0.0) & (t_ns < 0.5)).astype(np.float32)\n",
    "        mid   = ((t_ns >= 0.5) & (t_ns < 4.0)).astype(np.float32)\n",
    "        late  = ((t_ns >= 4.0) & (t_ns <= crop_t_max)).astype(np.float32)\n",
    "\n",
    "        masks = np.stack([early, mid, late], axis=0)  # (3,T)\n",
    "\n",
    "        if mode == \"early_mid_late\":\n",
    "            return dtof[:, None, :] * masks[None, :, :]  # (N,3,T)\n",
    "\n",
    "        if mode == \"hybrid_4ch\":\n",
    "            full = dtof[:, None, :]                         # (N,1,T)\n",
    "            gated = dtof[:, None, :] * masks[None, :, :]    # (N,3,T)\n",
    "            return np.concatenate([full, gated], axis=1)    # (N,4,T)\n",
    "\n",
    "        raise ValueError(f\"Unknown channel_mode: {mode}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.signals[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e584e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for 1D DTOF signals with flexible input channels.\n",
    "\n",
    "    Channel counts (C) depend on:\n",
    "        channel_mode:\n",
    "            - \"single\"         -> 1\n",
    "            - \"early_mid_late\" -> 3\n",
    "            - \"hybrid_4ch\"     -> 4\n",
    "        input_rep:\n",
    "            - \"raw\" / \"log\"    -> multiplier 1\n",
    "            - \"raw_log\"        -> multiplier 2\n",
    "\n",
    "    So:\n",
    "        C = base_C * (2 if input_rep == \"raw_log\" else 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict, input_length: int = 3000, output_dim: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        base_C = {\"single\": 1, \"early_mid_late\": 3, \"hybrid_4ch\": 4}[cfg[\"channel_mode\"]]\n",
    "        mult = 2 if cfg.get(\"input_rep\", \"log\") == \"raw_log\" else 1\n",
    "        in_channels = base_C * mult\n",
    "\n",
    "        # Convolution blocks\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=7, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Compute flattened feature size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_length)\n",
    "            feat = self._forward_features(dummy)\n",
    "            self.flatten_dim = feat.shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        # Block 1\n",
    "        x = self.pool1(self.act(self.bn1(self.conv1(x))))\n",
    "        # Block 2\n",
    "        x = self.pool2(self.act(self.bn2(self.conv2(x))))\n",
    "        # Block 3\n",
    "        x = self.pool3(self.act(self.bn3(self.conv3(x))))\n",
    "        # Flatten\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a581ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    save_path=None,\n",
    "    eps=1e-12,\n",
    "    print_every=1,\n",
    "    exp_clip=20.0,   # exp(20) ~ 4.85e8, safety for overflow\n",
    "    patience = 20, \n",
    "    min_delta = 0.0, \n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # -------------------------\n",
    "        # TRAIN\n",
    "        # -------------------------\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        tr_sum_sq = torch.zeros(2, device=device)\n",
    "        tr_count = 0\n",
    "\n",
    "        for signals, labels in train_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device).float()                 # (B,2)\n",
    "\n",
    "            log_labels = torch.log(labels.clamp_min(eps))      # (B,2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds_log = model(signals).view_as(log_labels)     # (B,2)\n",
    "            loss = loss_fn(preds_log, log_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # RMSE in original units\n",
    "            with torch.no_grad():\n",
    "                preds_lin = torch.exp(preds_log.clamp(-exp_clip, exp_clip))\n",
    "                err = preds_lin - labels\n",
    "                tr_sum_sq += (err ** 2).sum(dim=0)\n",
    "                tr_count += labels.shape[0]\n",
    "\n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        train_rmse = torch.sqrt(tr_sum_sq / max(1, tr_count)).detach().cpu().numpy()\n",
    "\n",
    "        # -------------------------\n",
    "        # VALIDATE\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "        va_sum_sq = torch.zeros(2, device=device)\n",
    "        va_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in val_loader:\n",
    "                signals = signals.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                log_labels = torch.log(labels.clamp_min(eps))\n",
    "                preds_log = model(signals).view_as(log_labels)\n",
    "\n",
    "                loss = loss_fn(preds_log, log_labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                preds_lin = torch.exp(preds_log.clamp(-exp_clip, exp_clip))\n",
    "                err = preds_lin - labels\n",
    "                va_sum_sq += (err ** 2).sum(dim=0)\n",
    "                va_count += labels.shape[0]\n",
    "\n",
    "        val_loss = val_running_loss / max(1, len(val_loader))\n",
    "        val_rmse = torch.sqrt(va_sum_sq / max(1, va_count)).detach().cpu().numpy()\n",
    "\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Train Loss (log-MSE): {train_loss:.6f} | Train RMSE [mua, mus]: {train_rmse}\")\n",
    "            print(f\"Val   Loss (log-MSE): {val_loss:.6f} | Val   RMSE [mua, mus]: {val_rmse}\")\n",
    "\n",
    "        if val_loss < (best_val_loss - min_delta):\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            if save_path is not None:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\" -> Best validation so far, saved.\")\n",
    "            else: \n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience: \n",
    "                    print(f\"\\nEarly stopping at epoch {epoch + 1}:\"\n",
    "                          f\"no val improvement for {patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1bb93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_loaders(dataset, batch_size=32, val_frac=0.2, seed=42, shuffle_train=True):\n",
    "    n = len(dataset)\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    split = int(n * (1 - val_frac))\n",
    "    train_idx = idx[:split]\n",
    "    val_idx = idx[split:]\n",
    "\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5483d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: torch.Size([8, 3000]) torch.Size([2])\n",
      "model out: torch.Size([1, 2])\n",
      "signals: torch.Size([32, 8, 3000])\n",
      "labels: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "# Trial run\n",
    "\n",
    "matlab_path = \"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/dataset_homo_small.mat\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = {\n",
    "        \"crop_t_max\": 6.0,\n",
    "        \"sg_window\": frame_length,\n",
    "        \"sg_order\": order,\n",
    "        \"eps\": 1e-12,\n",
    "        \"channel_mode\": \"hybrid_4ch\",  # \"single\" | \"early_mid_late\" | \"hybrid_4ch\"\n",
    "        \"input_rep\": \"raw_log\",        # \"raw\" | \"log\" | \"raw_log\"\n",
    "    }\n",
    "\n",
    "    ds = DTOFDataset(matlab_path, cfg)\n",
    "    x, y = ds[0]\n",
    "    print(\"sample:\", x.shape, y.shape)\n",
    "\n",
    "    model = Net(cfg, input_length=ds.T, output_dim=2).to(device)\n",
    "    out = model(x.unsqueeze(0))\n",
    "    print(\"model out:\", out.shape)\n",
    "\n",
    "    train_loader, val_loader = make_train_val_loaders(ds, batch_size=32, val_frac=0.2, seed=42)\n",
    "\n",
    "    signals, labels = next(iter(train_loader))\n",
    "    print(\"signals:\", signals.shape)  # (B, C, T)\n",
    "    print(\"labels:\", labels.shape)    # (B, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414ed861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator: \n",
    "    \"\"\"\n",
    "    Evaluates a trained model that outputs log(mua), log(mus).\n",
    "    Reports MAE/RMSE in original units by exp().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device, eps=1e-12):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.eps = eps\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        all_preds_lin = []\n",
    "        all_labels_lin = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in data_loader:\n",
    "                signals = signals.to(self.device)\n",
    "                labels = labels.to(self.device).float()              # (B,2) linear\n",
    "\n",
    "                preds_log = self.model(signals)                      # (B,2) log\n",
    "                preds_log = preds_log.view_as(labels)\n",
    "\n",
    "                preds_lin = torch.exp(preds_log)                     # back to linear\n",
    "\n",
    "                all_preds_lin.append(preds_lin.cpu())\n",
    "                all_labels_lin.append(labels.cpu())\n",
    "\n",
    "        preds = torch.cat(all_preds_lin, dim=0)\n",
    "        labs  = torch.cat(all_labels_lin, dim=0)\n",
    "\n",
    "        abs_err = torch.abs(preds - labs)\n",
    "        sq_err  = (preds - labs) ** 2\n",
    "\n",
    "        mae = abs_err.mean(dim=0)\n",
    "        rmse = torch.sqrt(sq_err.mean(dim=0))\n",
    "\n",
    "        return {\n",
    "            \"MAE\": mae.numpy(),\n",
    "            \"RMSE\": rmse.numpy(),\n",
    "            \"preds\": preds.numpy(),\n",
    "            \"labels\": labs.numpy(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total samples: 500\n",
      "Train samples: 400\n",
      "Val samples  : 100\n",
      "Signal shape : torch.Size([8, 3000]) Label shape: torch.Size([2])\n",
      "steps_per_epoch: 13\n",
      "target_steps   : 50000\n",
      "num_epochs     : 3847\n",
      "Pre-train RMSE train [mua, mus]: [ 1.2259294 13.752475 ]\n",
      "Pre-train RMSE val   [mua, mus]: [ 1.1722105 13.688775 ]\n",
      "\n",
      "Epoch 1/3847\n",
      "Train Loss (log-MSE): 2.224561 | Train RMSE [mua, mus]: [ 0.30347875 10.434857  ]\n",
      "Val   Loss (log-MSE): 3.912572 | Val   RMSE [mua, mus]: [ 0.19102694 12.617171  ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 2/3847\n",
      "Train Loss (log-MSE): 0.195658 | Train RMSE [mua, mus]: [ 0.01387672 11.672562  ]\n",
      "Val   Loss (log-MSE): 0.645825 | Val   RMSE [mua, mus]: [0.03196168 8.192657  ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 3/3847\n",
      "Train Loss (log-MSE): 0.235209 | Train RMSE [mua, mus]: [0.02188006 3.9205337 ]\n",
      "Val   Loss (log-MSE): 0.134619 | Val   RMSE [mua, mus]: [0.01512351 4.3076487 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 4/3847\n",
      "Train Loss (log-MSE): 0.165115 | Train RMSE [mua, mus]: [0.01179801 6.245989  ]\n",
      "Val   Loss (log-MSE): 0.097756 | Val   RMSE [mua, mus]: [0.01638312 4.857108  ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 5/3847\n",
      "Train Loss (log-MSE): 0.228975 | Train RMSE [mua, mus]: [0.01684635 9.49087   ]\n",
      "Val   Loss (log-MSE): 0.090927 | Val   RMSE [mua, mus]: [0.00824246 3.0885224 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 6/3847\n",
      "Train Loss (log-MSE): 0.201368 | Train RMSE [mua, mus]: [0.01465518 7.236531  ]\n",
      "Val   Loss (log-MSE): 0.046738 | Val   RMSE [mua, mus]: [0.00651661 4.4785547 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 7/3847\n",
      "Train Loss (log-MSE): 0.133945 | Train RMSE [mua, mus]: [0.01306405 6.6213546 ]\n",
      "Val   Loss (log-MSE): 0.052499 | Val   RMSE [mua, mus]: [3.3213543e-03 4.3362684e+00]\n",
      "\n",
      "Epoch 8/3847\n",
      "Train Loss (log-MSE): 0.084533 | Train RMSE [mua, mus]: [0.00864713 4.9785485 ]\n",
      "Val   Loss (log-MSE): 0.074585 | Val   RMSE [mua, mus]: [0.01375182 4.8336487 ]\n",
      "\n",
      "Epoch 9/3847\n",
      "Train Loss (log-MSE): 0.060395 | Train RMSE [mua, mus]: [0.0123454 3.1595237]\n",
      "Val   Loss (log-MSE): 0.023451 | Val   RMSE [mua, mus]: [0.00431751 3.257779  ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 10/3847\n",
      "Train Loss (log-MSE): 0.053977 | Train RMSE [mua, mus]: [0.00747189 3.2486243 ]\n",
      "Val   Loss (log-MSE): 0.056745 | Val   RMSE [mua, mus]: [0.00604172 3.0451503 ]\n",
      "\n",
      "Epoch 11/3847\n",
      "Train Loss (log-MSE): 0.065831 | Train RMSE [mua, mus]: [0.0101781 3.766813 ]\n",
      "Val   Loss (log-MSE): 0.032062 | Val   RMSE [mua, mus]: [0.00780967 2.5899746 ]\n",
      "\n",
      "Epoch 12/3847\n",
      "Train Loss (log-MSE): 0.077018 | Train RMSE [mua, mus]: [0.01099706 2.8431888 ]\n",
      "Val   Loss (log-MSE): 0.068212 | Val   RMSE [mua, mus]: [0.01086178 2.283894  ]\n",
      "\n",
      "Epoch 13/3847\n",
      "Train Loss (log-MSE): 0.039996 | Train RMSE [mua, mus]: [0.00647757 2.7547328 ]\n",
      "Val   Loss (log-MSE): 0.008526 | Val   RMSE [mua, mus]: [0.00295053 2.2356768 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 14/3847\n",
      "Train Loss (log-MSE): 0.033700 | Train RMSE [mua, mus]: [0.00634078 3.2167637 ]\n",
      "Val   Loss (log-MSE): 0.017597 | Val   RMSE [mua, mus]: [0.00307504 2.692015  ]\n",
      "\n",
      "Epoch 15/3847\n",
      "Train Loss (log-MSE): 0.057775 | Train RMSE [mua, mus]: [0.00687797 3.1444814 ]\n",
      "Val   Loss (log-MSE): 0.060712 | Val   RMSE [mua, mus]: [0.01150219 1.9965976 ]\n",
      "\n",
      "Epoch 16/3847\n",
      "Train Loss (log-MSE): 0.093546 | Train RMSE [mua, mus]: [0.01174289 3.25081   ]\n",
      "Val   Loss (log-MSE): 0.035751 | Val   RMSE [mua, mus]: [0.00681087 3.7160962 ]\n",
      "\n",
      "Epoch 17/3847\n",
      "Train Loss (log-MSE): 0.046769 | Train RMSE [mua, mus]: [0.00851904 2.296358  ]\n",
      "Val   Loss (log-MSE): 0.033499 | Val   RMSE [mua, mus]: [0.00518647 2.2833593 ]\n",
      "\n",
      "Epoch 18/3847\n",
      "Train Loss (log-MSE): 0.063282 | Train RMSE [mua, mus]: [0.00835548 2.4944348 ]\n",
      "Val   Loss (log-MSE): 0.020548 | Val   RMSE [mua, mus]: [0.00589283 2.038018  ]\n",
      "\n",
      "Epoch 19/3847\n",
      "Train Loss (log-MSE): 0.133632 | Train RMSE [mua, mus]: [0.01564401 4.4862823 ]\n",
      "Val   Loss (log-MSE): 0.130661 | Val   RMSE [mua, mus]: [0.01279839 4.2556095 ]\n",
      "\n",
      "Epoch 20/3847\n",
      "Train Loss (log-MSE): 0.075569 | Train RMSE [mua, mus]: [0.01113336 4.139139  ]\n",
      "Val   Loss (log-MSE): 0.013645 | Val   RMSE [mua, mus]: [0.00504044 2.4672253 ]\n",
      "\n",
      "Epoch 21/3847\n",
      "Train Loss (log-MSE): 0.033021 | Train RMSE [mua, mus]: [0.00634192 2.0964305 ]\n",
      "Val   Loss (log-MSE): 0.021790 | Val   RMSE [mua, mus]: [0.00388404 2.7061696 ]\n",
      "\n",
      "Epoch 22/3847\n",
      "Train Loss (log-MSE): 0.027632 | Train RMSE [mua, mus]: [0.00551253 2.7807722 ]\n",
      "Val   Loss (log-MSE): 0.018411 | Val   RMSE [mua, mus]: [0.0061159 1.9020281]\n",
      "\n",
      "Epoch 23/3847\n",
      "Train Loss (log-MSE): 0.040547 | Train RMSE [mua, mus]: [0.0062246 3.3429384]\n",
      "Val   Loss (log-MSE): 0.055498 | Val   RMSE [mua, mus]: [0.00774886 3.6432817 ]\n",
      "\n",
      "Epoch 24/3847\n",
      "Train Loss (log-MSE): 0.068962 | Train RMSE [mua, mus]: [0.00703152 3.6593876 ]\n",
      "Val   Loss (log-MSE): 0.041994 | Val   RMSE [mua, mus]: [0.00590809 2.6031055 ]\n",
      "\n",
      "Epoch 25/3847\n",
      "Train Loss (log-MSE): 0.060836 | Train RMSE [mua, mus]: [0.0104811 3.3928022]\n",
      "Val   Loss (log-MSE): 0.023612 | Val   RMSE [mua, mus]: [0.00789674 2.9004495 ]\n",
      "\n",
      "Epoch 26/3847\n",
      "Train Loss (log-MSE): 0.038679 | Train RMSE [mua, mus]: [0.01010159 3.3548915 ]\n",
      "Val   Loss (log-MSE): 0.013143 | Val   RMSE [mua, mus]: [0.00499449 1.4057148 ]\n",
      "\n",
      "Epoch 27/3847\n",
      "Train Loss (log-MSE): 0.020202 | Train RMSE [mua, mus]: [0.00507879 2.317016  ]\n",
      "Val   Loss (log-MSE): 0.010492 | Val   RMSE [mua, mus]: [0.0033714 1.6966827]\n",
      "\n",
      "Epoch 28/3847\n",
      "Train Loss (log-MSE): 0.028512 | Train RMSE [mua, mus]: [0.00390866 2.6355588 ]\n",
      "Val   Loss (log-MSE): 0.010580 | Val   RMSE [mua, mus]: [0.0019219 1.4812931]\n",
      "\n",
      "Epoch 29/3847\n",
      "Train Loss (log-MSE): 0.025101 | Train RMSE [mua, mus]: [0.00535489 2.3765404 ]\n",
      "Val   Loss (log-MSE): 0.009548 | Val   RMSE [mua, mus]: [0.00507623 1.6224878 ]\n",
      "\n",
      "Epoch 30/3847\n",
      "Train Loss (log-MSE): 0.020468 | Train RMSE [mua, mus]: [0.00606953 2.67081   ]\n",
      "Val   Loss (log-MSE): 0.009295 | Val   RMSE [mua, mus]: [1.9965817e-03 2.3749485e+00]\n",
      "\n",
      "Epoch 31/3847\n",
      "Train Loss (log-MSE): 0.026980 | Train RMSE [mua, mus]: [0.00582167 2.2683003 ]\n",
      "Val   Loss (log-MSE): 0.035049 | Val   RMSE [mua, mus]: [0.00735738 2.9946692 ]\n",
      "\n",
      "Epoch 32/3847\n",
      "Train Loss (log-MSE): 0.037486 | Train RMSE [mua, mus]: [0.00642107 3.2185957 ]\n",
      "Val   Loss (log-MSE): 0.056434 | Val   RMSE [mua, mus]: [0.00830146 4.0916867 ]\n",
      "\n",
      "Epoch 33/3847\n",
      "Train Loss (log-MSE): 0.037145 | Train RMSE [mua, mus]: [0.00834996 3.3939612 ]\n",
      "Val   Loss (log-MSE): 0.006979 | Val   RMSE [mua, mus]: [0.00346651 1.2918453 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 34/3847\n",
      "Train Loss (log-MSE): 0.022512 | Train RMSE [mua, mus]: [0.0068637 2.0881248]\n",
      "Val   Loss (log-MSE): 0.007975 | Val   RMSE [mua, mus]: [0.00197345 1.3450844 ]\n",
      "\n",
      "Epoch 35/3847\n",
      "Train Loss (log-MSE): 0.026672 | Train RMSE [mua, mus]: [0.00554497 2.299202  ]\n",
      "Val   Loss (log-MSE): 0.014428 | Val   RMSE [mua, mus]: [0.00255604 1.1924984 ]\n",
      "\n",
      "Epoch 36/3847\n",
      "Train Loss (log-MSE): 0.020494 | Train RMSE [mua, mus]: [0.00491789 1.7680386 ]\n",
      "Val   Loss (log-MSE): 0.074126 | Val   RMSE [mua, mus]: [0.0075766 3.0451293]\n",
      "\n",
      "Epoch 37/3847\n",
      "Train Loss (log-MSE): 0.041428 | Train RMSE [mua, mus]: [0.00834745 2.5284586 ]\n",
      "Val   Loss (log-MSE): 0.114712 | Val   RMSE [mua, mus]: [0.01443904 5.288742  ]\n",
      "\n",
      "Epoch 38/3847\n",
      "Train Loss (log-MSE): 0.058209 | Train RMSE [mua, mus]: [0.01546946 3.5911145 ]\n",
      "Val   Loss (log-MSE): 0.013891 | Val   RMSE [mua, mus]: [0.00654938 2.0611217 ]\n",
      "\n",
      "Epoch 39/3847\n",
      "Train Loss (log-MSE): 0.043071 | Train RMSE [mua, mus]: [0.0082553 3.1498067]\n",
      "Val   Loss (log-MSE): 0.020338 | Val   RMSE [mua, mus]: [0.00630801 3.2732542 ]\n",
      "\n",
      "Epoch 40/3847\n",
      "Train Loss (log-MSE): 0.041535 | Train RMSE [mua, mus]: [0.00981565 2.4276087 ]\n",
      "Val   Loss (log-MSE): 0.047214 | Val   RMSE [mua, mus]: [0.01512139 1.7735304 ]\n",
      "\n",
      "Epoch 41/3847\n",
      "Train Loss (log-MSE): 0.037003 | Train RMSE [mua, mus]: [0.00785322 2.9461668 ]\n",
      "Val   Loss (log-MSE): 0.012357 | Val   RMSE [mua, mus]: [0.00267877 2.014784  ]\n",
      "\n",
      "Epoch 42/3847\n",
      "Train Loss (log-MSE): 0.026319 | Train RMSE [mua, mus]: [0.00580925 1.9831358 ]\n",
      "Val   Loss (log-MSE): 0.008346 | Val   RMSE [mua, mus]: [0.0031985 1.8566492]\n",
      "\n",
      "Epoch 43/3847\n",
      "Train Loss (log-MSE): 0.023650 | Train RMSE [mua, mus]: [0.00408004 2.3815787 ]\n",
      "Val   Loss (log-MSE): 0.011319 | Val   RMSE [mua, mus]: [0.00250962 1.7993381 ]\n",
      "\n",
      "Epoch 44/3847\n",
      "Train Loss (log-MSE): 0.014558 | Train RMSE [mua, mus]: [0.00414201 2.332163  ]\n",
      "Val   Loss (log-MSE): 0.009262 | Val   RMSE [mua, mus]: [0.00238891 1.5188384 ]\n",
      "\n",
      "Epoch 45/3847\n",
      "Train Loss (log-MSE): 0.021160 | Train RMSE [mua, mus]: [0.01154281 1.6914915 ]\n",
      "Val   Loss (log-MSE): 0.020077 | Val   RMSE [mua, mus]: [0.01062085 1.5532948 ]\n",
      "\n",
      "Epoch 46/3847\n",
      "Train Loss (log-MSE): 0.030954 | Train RMSE [mua, mus]: [0.00951183 1.8173114 ]\n",
      "Val   Loss (log-MSE): 0.014457 | Val   RMSE [mua, mus]: [0.0026243 1.509344 ]\n",
      "\n",
      "Epoch 47/3847\n",
      "Train Loss (log-MSE): 0.025159 | Train RMSE [mua, mus]: [0.00734593 2.3676758 ]\n",
      "Val   Loss (log-MSE): 0.013408 | Val   RMSE [mua, mus]: [0.00280817 1.4474403 ]\n",
      "\n",
      "Epoch 48/3847\n",
      "Train Loss (log-MSE): 0.016104 | Train RMSE [mua, mus]: [0.0040715 1.8720033]\n",
      "Val   Loss (log-MSE): 0.005180 | Val   RMSE [mua, mus]: [0.00250047 1.7298687 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 49/3847\n",
      "Train Loss (log-MSE): 0.012625 | Train RMSE [mua, mus]: [0.00381369 1.5030601 ]\n",
      "Val   Loss (log-MSE): 0.011096 | Val   RMSE [mua, mus]: [0.00357026 1.49884   ]\n",
      "\n",
      "Epoch 50/3847\n",
      "Train Loss (log-MSE): 0.040070 | Train RMSE [mua, mus]: [0.00556675 2.3250651 ]\n",
      "Val   Loss (log-MSE): 0.025319 | Val   RMSE [mua, mus]: [0.00254987 1.9202294 ]\n",
      "\n",
      "Epoch 51/3847\n",
      "Train Loss (log-MSE): 0.036590 | Train RMSE [mua, mus]: [0.00698311 2.9786546 ]\n",
      "Val   Loss (log-MSE): 0.019676 | Val   RMSE [mua, mus]: [0.00540515 1.4311154 ]\n",
      "\n",
      "Epoch 52/3847\n",
      "Train Loss (log-MSE): 0.038862 | Train RMSE [mua, mus]: [0.00598121 2.9836802 ]\n",
      "Val   Loss (log-MSE): 0.010033 | Val   RMSE [mua, mus]: [0.00181939 1.4949212 ]\n",
      "\n",
      "Epoch 53/3847\n",
      "Train Loss (log-MSE): 0.031127 | Train RMSE [mua, mus]: [0.00603322 3.4536552 ]\n",
      "Val   Loss (log-MSE): 0.031088 | Val   RMSE [mua, mus]: [3.6376403e-03 3.8377721e+00]\n",
      "\n",
      "Epoch 54/3847\n",
      "Train Loss (log-MSE): 0.014426 | Train RMSE [mua, mus]: [0.00350199 1.8928521 ]\n",
      "Val   Loss (log-MSE): 0.009656 | Val   RMSE [mua, mus]: [0.00249178 2.0581214 ]\n",
      "\n",
      "Epoch 55/3847\n",
      "Train Loss (log-MSE): 0.014366 | Train RMSE [mua, mus]: [0.00410926 1.9856038 ]\n",
      "Val   Loss (log-MSE): 0.012104 | Val   RMSE [mua, mus]: [0.00227931 2.0159416 ]\n",
      "\n",
      "Epoch 56/3847\n",
      "Train Loss (log-MSE): 0.011656 | Train RMSE [mua, mus]: [0.0037783 1.6403593]\n",
      "Val   Loss (log-MSE): 0.005713 | Val   RMSE [mua, mus]: [0.00174955 1.580829  ]\n",
      "\n",
      "Epoch 57/3847\n",
      "Train Loss (log-MSE): 0.009169 | Train RMSE [mua, mus]: [0.00279193 1.4645008 ]\n",
      "Val   Loss (log-MSE): 0.011579 | Val   RMSE [mua, mus]: [0.0021003 1.6091762]\n",
      "\n",
      "Epoch 58/3847\n",
      "Train Loss (log-MSE): 0.015266 | Train RMSE [mua, mus]: [0.00421296 1.9808844 ]\n",
      "Val   Loss (log-MSE): 0.007992 | Val   RMSE [mua, mus]: [0.00280726 1.3538576 ]\n",
      "\n",
      "Epoch 59/3847\n",
      "Train Loss (log-MSE): 0.009385 | Train RMSE [mua, mus]: [0.003294  1.9260377]\n",
      "Val   Loss (log-MSE): 0.004832 | Val   RMSE [mua, mus]: [1.2273575e-03 1.3380815e+00]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 60/3847\n",
      "Train Loss (log-MSE): 0.008240 | Train RMSE [mua, mus]: [0.0033014 1.4036688]\n",
      "Val   Loss (log-MSE): 0.004485 | Val   RMSE [mua, mus]: [0.00344073 1.6031885 ]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 61/3847\n",
      "Train Loss (log-MSE): 0.010926 | Train RMSE [mua, mus]: [0.0038002 1.3625606]\n",
      "Val   Loss (log-MSE): 0.009159 | Val   RMSE [mua, mus]: [0.00235377 2.0275335 ]\n",
      "\n",
      "Epoch 62/3847\n",
      "Train Loss (log-MSE): 0.008511 | Train RMSE [mua, mus]: [0.00262134 1.5524031 ]\n",
      "Val   Loss (log-MSE): 0.003151 | Val   RMSE [mua, mus]: [1.0601901e-03 1.0667694e+00]\n",
      " -> Best validation so far, saved.\n",
      "\n",
      "Epoch 63/3847\n",
      "Train Loss (log-MSE): 0.006265 | Train RMSE [mua, mus]: [0.00251068 1.2411793 ]\n",
      "Val   Loss (log-MSE): 0.004623 | Val   RMSE [mua, mus]: [0.00177379 1.0554382 ]\n",
      "\n",
      "Epoch 64/3847\n",
      "Train Loss (log-MSE): 0.007663 | Train RMSE [mua, mus]: [0.00302185 1.3837868 ]\n",
      "Val   Loss (log-MSE): 0.014747 | Val   RMSE [mua, mus]: [0.00221081 1.8169968 ]\n",
      "\n",
      "Epoch 65/3847\n",
      "Train Loss (log-MSE): 0.014783 | Train RMSE [mua, mus]: [0.00559696 1.8568116 ]\n",
      "Val   Loss (log-MSE): 0.011088 | Val   RMSE [mua, mus]: [0.00392718 1.486886  ]\n",
      "\n",
      "Epoch 66/3847\n",
      "Train Loss (log-MSE): 0.007974 | Train RMSE [mua, mus]: [0.00358147 1.2992955 ]\n",
      "Val   Loss (log-MSE): 0.006296 | Val   RMSE [mua, mus]: [0.00289768 1.443136  ]\n",
      "\n",
      "Epoch 67/3847\n",
      "Train Loss (log-MSE): 0.008646 | Train RMSE [mua, mus]: [0.00295407 1.3398609 ]\n",
      "Val   Loss (log-MSE): 0.010146 | Val   RMSE [mua, mus]: [1.7810399e-03 1.8359168e+00]\n",
      "\n",
      "Epoch 68/3847\n",
      "Train Loss (log-MSE): 0.009196 | Train RMSE [mua, mus]: [0.00301627 1.5968403 ]\n",
      "Val   Loss (log-MSE): 0.009674 | Val   RMSE [mua, mus]: [0.00244939 1.0958107 ]\n",
      "\n",
      "Epoch 69/3847\n",
      "Train Loss (log-MSE): 0.010870 | Train RMSE [mua, mus]: [0.00187519 1.7187715 ]\n",
      "Val   Loss (log-MSE): 0.008812 | Val   RMSE [mua, mus]: [0.00413951 1.0329715 ]\n",
      "\n",
      "Epoch 70/3847\n",
      "Train Loss (log-MSE): 0.016234 | Train RMSE [mua, mus]: [0.00395699 2.2322624 ]\n",
      "Val   Loss (log-MSE): 0.005911 | Val   RMSE [mua, mus]: [0.00259763 1.9124346 ]\n",
      "\n",
      "Epoch 71/3847\n",
      "Train Loss (log-MSE): 0.006921 | Train RMSE [mua, mus]: [0.00241715 1.4271047 ]\n",
      "Val   Loss (log-MSE): 0.004573 | Val   RMSE [mua, mus]: [0.00179793 1.2499741 ]\n",
      "\n",
      "Epoch 72/3847\n",
      "Train Loss (log-MSE): 0.007137 | Train RMSE [mua, mus]: [0.00296061 1.353511  ]\n",
      "Val   Loss (log-MSE): 0.005989 | Val   RMSE [mua, mus]: [0.00222926 1.3068851 ]\n",
      "\n",
      "Epoch 73/3847\n",
      "Train Loss (log-MSE): 0.009479 | Train RMSE [mua, mus]: [0.00448492 1.3111062 ]\n",
      "Val   Loss (log-MSE): 0.009328 | Val   RMSE [mua, mus]: [0.00201791 0.97444624]\n",
      "\n",
      "Epoch 74/3847\n",
      "Train Loss (log-MSE): 0.010074 | Train RMSE [mua, mus]: [0.00531053 1.3447466 ]\n",
      "Val   Loss (log-MSE): 0.013314 | Val   RMSE [mua, mus]: [0.00572982 0.945494  ]\n",
      "\n",
      "Epoch 75/3847\n",
      "Train Loss (log-MSE): 0.012031 | Train RMSE [mua, mus]: [0.00500167 1.1959502 ]\n",
      "Val   Loss (log-MSE): 0.010012 | Val   RMSE [mua, mus]: [0.00217385 1.662117  ]\n",
      "\n",
      "Epoch 76/3847\n",
      "Train Loss (log-MSE): 0.011355 | Train RMSE [mua, mus]: [0.00403852 1.9390941 ]\n",
      "Val   Loss (log-MSE): 0.008365 | Val   RMSE [mua, mus]: [1.4571284e-03 2.7053180e+00]\n",
      "\n",
      "Epoch 77/3847\n",
      "Train Loss (log-MSE): 0.015964 | Train RMSE [mua, mus]: [0.00390946 2.0832179 ]\n",
      "Val   Loss (log-MSE): 0.011206 | Val   RMSE [mua, mus]: [0.00435888 1.5404661 ]\n",
      "\n",
      "Epoch 78/3847\n",
      "Train Loss (log-MSE): 0.012876 | Train RMSE [mua, mus]: [0.00345668 1.7028133 ]\n",
      "Val   Loss (log-MSE): 0.004616 | Val   RMSE [mua, mus]: [0.00363745 0.9202649 ]\n",
      "\n",
      "Epoch 79/3847\n",
      "Train Loss (log-MSE): 0.011145 | Train RMSE [mua, mus]: [0.00353399 1.7222673 ]\n",
      "Val   Loss (log-MSE): 0.008316 | Val   RMSE [mua, mus]: [1.6670747e-03 1.9687320e+00]\n",
      "\n",
      "Epoch 80/3847\n",
      "Train Loss (log-MSE): 0.006747 | Train RMSE [mua, mus]: [0.00292891 1.3125466 ]\n",
      "Val   Loss (log-MSE): 0.003315 | Val   RMSE [mua, mus]: [1.1495315e-03 1.1507992e+00]\n",
      "\n",
      "Epoch 81/3847\n",
      "Train Loss (log-MSE): 0.008264 | Train RMSE [mua, mus]: [0.00340459 1.3125395 ]\n",
      "Val   Loss (log-MSE): 0.014676 | Val   RMSE [mua, mus]: [0.00380066 1.3560542 ]\n",
      "\n",
      "Epoch 82/3847\n",
      "Train Loss (log-MSE): 0.007481 | Train RMSE [mua, mus]: [0.00300375 1.3095167 ]\n",
      "Val   Loss (log-MSE): 0.004024 | Val   RMSE [mua, mus]: [0.00279097 1.2812274 ]\n",
      "\n",
      "Epoch 83/3847\n",
      "Train Loss (log-MSE): 0.008312 | Train RMSE [mua, mus]: [0.00454471 1.6611851 ]\n",
      "Val   Loss (log-MSE): 0.008325 | Val   RMSE [mua, mus]: [0.00379096 1.9199878 ]\n",
      "\n",
      "Epoch 84/3847\n",
      "Train Loss (log-MSE): 0.006921 | Train RMSE [mua, mus]: [0.0030649 1.4030371]\n",
      "Val   Loss (log-MSE): 0.007114 | Val   RMSE [mua, mus]: [0.00201939 1.7057493 ]\n",
      "\n",
      "Epoch 85/3847\n",
      "Train Loss (log-MSE): 0.007927 | Train RMSE [mua, mus]: [0.00235328 1.6315385 ]\n",
      "Val   Loss (log-MSE): 0.009633 | Val   RMSE [mua, mus]: [0.00215107 0.88313925]\n",
      "\n",
      "Epoch 86/3847\n",
      "Train Loss (log-MSE): 0.009101 | Train RMSE [mua, mus]: [0.00243731 1.9498389 ]\n",
      "Val   Loss (log-MSE): 0.010874 | Val   RMSE [mua, mus]: [1.5206420e-03 2.3728762e+00]\n",
      "\n",
      "Epoch 87/3847\n",
      "Train Loss (log-MSE): 0.011102 | Train RMSE [mua, mus]: [0.00351371 1.7150571 ]\n",
      "Val   Loss (log-MSE): 0.014491 | Val   RMSE [mua, mus]: [0.00728309 1.6945523 ]\n",
      "\n",
      "Epoch 88/3847\n",
      "Train Loss (log-MSE): 0.009657 | Train RMSE [mua, mus]: [0.00385681 1.8236418 ]\n",
      "Val   Loss (log-MSE): 0.008029 | Val   RMSE [mua, mus]: [0.0017865 1.3207682]\n",
      "\n",
      "Epoch 89/3847\n",
      "Train Loss (log-MSE): 0.009425 | Train RMSE [mua, mus]: [0.00333323 1.6723173 ]\n",
      "Val   Loss (log-MSE): 0.012143 | Val   RMSE [mua, mus]: [0.00601054 1.9867871 ]\n",
      "\n",
      "Epoch 90/3847\n",
      "Train Loss (log-MSE): 0.008386 | Train RMSE [mua, mus]: [0.00394151 1.4355108 ]\n",
      "Val   Loss (log-MSE): 0.004178 | Val   RMSE [mua, mus]: [0.00169803 1.435942  ]\n",
      "\n",
      "Epoch 91/3847\n",
      "Train Loss (log-MSE): 0.010565 | Train RMSE [mua, mus]: [0.00412474 1.3749408 ]\n",
      "Val   Loss (log-MSE): 0.007903 | Val   RMSE [mua, mus]: [0.00240584 1.2942113 ]\n",
      "\n",
      "Epoch 92/3847\n",
      "Train Loss (log-MSE): 0.009217 | Train RMSE [mua, mus]: [0.00248218 1.575698  ]\n",
      "Val   Loss (log-MSE): 0.021357 | Val   RMSE [mua, mus]: [2.2830949e-03 3.4301815e+00]\n",
      "\n",
      "Epoch 93/3847\n",
      "Train Loss (log-MSE): 0.008930 | Train RMSE [mua, mus]: [0.00300633 1.4513271 ]\n",
      "Val   Loss (log-MSE): 0.003942 | Val   RMSE [mua, mus]: [0.003326  0.9767317]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TRIAL RUN\n",
    "# dataset -> split -> auto epochs -> metrics sanity checks\n",
    "# ============================\n",
    "\n",
    "def make_train_val_loaders(dataset, batch_size=32, val_frac=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    split = int(n * (1 - val_frac))\n",
    "    train_idx = idx[:split]\n",
    "    val_idx = idx[split:]\n",
    "\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def quick_eval_lin_rmse(model, loader, device, eps=1e-12, exp_clip=20.0):\n",
    "    \"\"\"Compute RMSE in original units on a loader (model outputs log-space).\"\"\"\n",
    "    model.eval()\n",
    "    sum_sq = torch.zeros(2, device=device)\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).float()\n",
    "            ylog = torch.log(y.clamp_min(eps))\n",
    "            plog = model(x).view_as(ylog)\n",
    "            plin = torch.exp(plog.clamp(-exp_clip, exp_clip))\n",
    "            err = plin - y\n",
    "            sum_sq += (err ** 2).sum(dim=0)\n",
    "            n += y.shape[0]\n",
    "    return torch.sqrt(sum_sq / max(1, n)).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Paths + config\n",
    "# ----------------------------\n",
    "mat_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/dataset_homo_small.mat\"\n",
    "\n",
    "cfg = {\n",
    "    \"crop_t_max\": 6.0,\n",
    "    \"sg_window\": 21,\n",
    "    \"sg_order\": 1,\n",
    "    \"eps\": 1e-12,\n",
    "    \"channel_mode\": \"hybrid_4ch\",\n",
    "    \"input_rep\": \"raw_log\",\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Device\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset + split\n",
    "# ----------------------------\n",
    "dataset = DTOFDataset(mat_path, cfg)\n",
    "\n",
    "batch_size = 32\n",
    "val_frac = 0.2\n",
    "seed = 42\n",
    "\n",
    "train_loader, val_loader = make_train_val_loaders(dataset, batch_size=batch_size, val_frac=val_frac, seed=seed)\n",
    "\n",
    "N_total = len(dataset)\n",
    "N_train = len(train_loader.dataset)\n",
    "N_val = len(val_loader.dataset)\n",
    "\n",
    "print(\"Total samples:\", N_total)\n",
    "print(\"Train samples:\", N_train)\n",
    "print(\"Val samples  :\", N_val)\n",
    "print(\"Signal shape :\", dataset[0][0].shape, \"Label shape:\", dataset[0][1].shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Model + optimizer\n",
    "# ----------------------------\n",
    "model = Net(cfg, input_length=dataset.T, output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Choose epochs based on target optimiser steps\n",
    "# ----------------------------\n",
    "target_steps = 50_000   # adjust later (e.g., 20k for quick, 100k+ for full)\n",
    "steps_per_epoch = math.ceil(N_train / batch_size)\n",
    "\n",
    "num_epochs = math.ceil(target_steps / steps_per_epoch)\n",
    "\n",
    "print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "print(f\"target_steps   : {target_steps}\")\n",
    "print(f\"num_epochs     : {num_epochs}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Pre-train sanity RMSE (random model)\n",
    "# ----------------------------\n",
    "rmse0_train = quick_eval_lin_rmse(model, train_loader, device, eps=cfg[\"eps\"])\n",
    "rmse0_val = quick_eval_lin_rmse(model, val_loader, device, eps=cfg[\"eps\"])\n",
    "print(\"Pre-train RMSE train [mua, mus]:\", rmse0_train)\n",
    "print(\"Pre-train RMSE val   [mua, mus]:\", rmse0_val)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Train\n",
    "# ----------------------------\n",
    "_ = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    save_path=\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/CNN_initial_saved_pytorch_model_weights/best_trial_TRIAL.pt\",\n",
    "    eps=cfg[\"eps\"],\n",
    "    print_every=1,\n",
    "    # if you added early stopping:\n",
    "    # patience=20,\n",
    "    # min_delta=1e-4,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Post-train sanity RMSE\n",
    "# ----------------------------\n",
    "rmse1_train = quick_eval_lin_rmse(model, train_loader, device, eps=cfg[\"eps\"])\n",
    "rmse1_val = quick_eval_lin_rmse(model, val_loader, device, eps=cfg[\"eps\"])\n",
    "print(\"Post-train RMSE train [mua, mus]:\", rmse1_train)\n",
    "print(\"Post-train RMSE val   [mua, mus]:\", rmse1_val)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) One-batch forward shape check\n",
    "# ----------------------------\n",
    "signals, labels = next(iter(train_loader))\n",
    "out = model(signals.to(device))\n",
    "print(\"One-batch:\")\n",
    "print(\"  signals:\", signals.shape)\n",
    "print(\"  labels :\", labels.shape)\n",
    "print(\"  output :\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908676a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
