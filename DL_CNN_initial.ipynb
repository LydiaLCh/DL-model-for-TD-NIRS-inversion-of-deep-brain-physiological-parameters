{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b73156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import random \n",
    "import torch \n",
    "from torch import nn \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "import pandas as pd \n",
    "import scipy \n",
    "from scipy.signal import savgol_filter \n",
    "        \n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Savitzky Golay filter parameters \n",
    "order = 1\n",
    "frame_length = 21\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b358256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTOFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DTOF signals stored in a single CSV.\n",
    "\n",
    "    Each column (from column 1 onward) is a DTOF trace; column 0 is time_ns.\n",
    "    Preprocessing applied:\n",
    "    - Savitzky-Golay smoothing\n",
    "    - Negative value clipping to eps\n",
    "    - Standardisation (mean 0, std 1) per trace\n",
    "    - 3-channel temporal masking (early/mid/late)\n",
    "    - Labels parsed from column headers -> (mua, mus)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            csv_path,\n",
    "            labels = None,\n",
    "            window_length = frame_length,\n",
    "            polyorder = order,\n",
    "            eps = eps,\n",
    "    ):\n",
    "            super().__init__()\n",
    "            self.csv_path = csv_path\n",
    "            self.window_length = window_length\n",
    "            self.polyorder = polyorder\n",
    "            self.eps = eps\n",
    "\n",
    "            # Load the CSV\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Column 0 is time, rest are DTOFs\n",
    "            self.time_ns = df.iloc[:, 0].values\n",
    "            dtof_matrix = df.iloc[:, 1:].values\n",
    "\n",
    "            # Transpose so rows = signals, cols = time -> (N_signals, T)\n",
    "            dtof_matrix = dtof_matrix.T\n",
    "            self.n_signals, self.T = dtof_matrix.shape\n",
    "\n",
    "            dtof_smooth = savgol_filter(\n",
    "                dtof_matrix,\n",
    "                window_length=window_length,\n",
    "                polyorder=polyorder,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Clip negative values to eps\n",
    "            dtof_smooth[dtof_smooth < 0] = eps\n",
    "\n",
    "            # Standardisation (mean = 0, std = 1)\n",
    "            means = dtof_smooth.mean(axis=1, keepdims=True)\n",
    "            stds = dtof_smooth.std(axis=1, keepdims=True)\n",
    "            dtof_standardised = (dtof_smooth - means) / (stds + eps)\n",
    "\n",
    "            # Store as float32 numpy array\n",
    "            self.signals = dtof_standardised.astype(np.float32)\n",
    "\n",
    "            # Build 3 temporal masks: early, mid, late\n",
    "            t = self.time_ns\n",
    "            early_mask = ((t >= 0.0) & (t < 0.5)).astype(np.float32)\n",
    "            mid_mask = ((t >= 0.5) & (t < 4.0)).astype(np.float32)\n",
    "            late_mask = ((t >= 4.0) & (t < 6.0)).astype(np.float32)\n",
    "\n",
    "            # Stack -> (3, T) so we can broadcast with each DTOF\n",
    "            self.masks = np.stack([early_mask, mid_mask, late_mask], axis=0).astype(np.float32)\n",
    "\n",
    "            # Labels parsed from column headers (mua, mus). Allows overriding via labels arg.\n",
    "            header_labels = self._parse_header_labels(df.columns[1:])\n",
    "            if labels is None:\n",
    "                self.labels = header_labels\n",
    "            else:\n",
    "                labels_arr = np.asarray(labels, dtype=np.float32)\n",
    "                if len(labels_arr) != self.n_signals:\n",
    "                    raise ValueError(f\"labels length {len(labels_arr)} does not match number of signals {self.n_signals}\")\n",
    "                self.labels = labels_arr\n",
    "\n",
    "    def _parse_header_labels(self, columns):\n",
    "            parsed = []\n",
    "            for col in columns:\n",
    "                col_clean = col.strip()\n",
    "                match = re.search(r\"mua:\\s*([0-9.]+)\\s+mus:\\s*([0-9.]+)\", col_clean)\n",
    "                if not match:\n",
    "                    raise ValueError(f\"Could not parse mua/mus from column name '{col}'\")\n",
    "                mua_val = float(match.group(1))\n",
    "                mus_val = float(match.group(2))\n",
    "                parsed.append((mua_val, mus_val))\n",
    "            return np.asarray(parsed, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "            return self.n_signals\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get a single DTOF\n",
    "        dtof = self.signals[index]\n",
    "\n",
    "        # Apply the 3 masks -> (3, T)\n",
    "        channels = self.masks * dtof\n",
    "        signal = torch.from_numpy(channels)\n",
    "        target = torch.tensor(self.labels[index])\n",
    "        \n",
    "        return signal, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb24a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels = 3, input_length = 3000, output_dim = 2):\n",
    "        \"\"\"\n",
    "        CNN for 1D DTOF signals with 3 input channels (early / mid / late masks)\n",
    "        Blocks: [Conv1d -> BN -> ReLU -> MaxPool1d] x 3 -> Flatten -> FCs.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution blocks\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Compute the flattened feature size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_length)\n",
    "            feat = self._forward_features(dummy)\n",
    "            self.flatten_dim = feat.shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"Convolutional feature extractor followed by flatten.\"\"\"\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten to (batch, features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_channels = 3, time_points = 3000)\n",
    "        \"\"\"\n",
    "        x = self._forward_features(x) # (batch, flatten_dim)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a53976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, save_path = None): \n",
    "    \"\"\" \n",
    "    Train the CNN with a training + validation loop \n",
    "\n",
    "    Inputs: \n",
    "        model: instance of Net \n",
    "        train_loader: DataLoader for training set (yields signals, labels)\n",
    "        val_loader: DataLoader for validation set \n",
    "        loss_fun : loss function, e.g. nn.MSELoss()\n",
    "        optimiser: optimiser, e.g. torch.optim.Adam(...)\n",
    "        num_epochs: number of epochs to train \n",
    "        device: u\n",
    "        save_path: optional path to save best model, to use later when we develop more models (str or None)\n",
    "    \"\"\"\n",
    "    # Move model to device \n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        print(f\"\\nEpoch {epoch + 1}/ {num_epochs}\")\n",
    "\n",
    "        # TRAINING PHASE \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for signals, labels in train_loader: \n",
    "            # Move the batch to device \n",
    "            signals = signals.to(device) # (batch, 3, T = 3000)\n",
    "            labels = labels.to(device).float() # (batch,) or (batch, 1)\n",
    "\n",
    "            # Zero gradients \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass \n",
    "            preds = model(signals) # (batch, 1) or (batch, )\n",
    "            preds = preds.view_as(labels) # reshape the predictions to have the same shape as labels \n",
    "\n",
    "            # Loss\n",
    "            loss = loss_fn(preds, labels)\n",
    "            \n",
    "            # Backward + update \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signals, labels in val_loader: \n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(signals)\n",
    "            preds = preds.view_as(labels)\n",
    "\n",
    "            loss = loss_fn(preds, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss: \n",
    "        print(\" -> Best validation loss so far, saving the model.\")\n",
    "        best_val_loss = val_loss\n",
    "        if save_path is not None: \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bb93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, target, test_size = 0.2, shuffle = True, random_state = None):\n",
    "    \"\"\"\n",
    "    Splits the data and target lists into training and validation subsets. \n",
    "    \"\"\"\n",
    "\n",
    "    if len(data) != len(target): \n",
    "        raise ValueError(\"Data and target must have the same length.\")\n",
    "    \n",
    "    if shuffle: \n",
    "        if random_state is not None: \n",
    "            random.seed(random_state)\n",
    "        pairs = list(zip(data, target))\n",
    "        random.shuffle(pairs)\n",
    "        data, target = zip(*pairs)\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "\n",
    "    return (\n",
    "        data[:split_idx], # X_train\n",
    "        data[split_idx:], # X_val \n",
    "        target[:split_idx], # y_train\n",
    "        target[split_idx:] # y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbd8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_from_dtof_csv(csv_path, label_csv_path):\n",
    "    \"\"\"\n",
    "    Extract (mua, mus) labels from DTOF CSV column headers and save to a new CSV file.\n",
    "\n",
    "    Input: \n",
    "        csv_path : str\n",
    "            Path to the large DTOF CSV file (first column = time_ns, others = DTOFs).\n",
    "        label_csv_path : str\n",
    "            Where to save the generated labels CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # DTOF columns start from index 1\n",
    "    dtof_columns = df.columns[1:]\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for col in dtof_columns:\n",
    "        col_clean = col.strip()\n",
    "\n",
    "        # Regex matches text like \"mua: 0.015 mus: 0.75\"\n",
    "        match = re.search(r\"mua:\\s*([0-9.]+)\\s+mus:\\s*([0-9.]+)\", col_clean)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse mua/mus from column '{col}'\")\n",
    "\n",
    "        mua_val = float(match.group(1))\n",
    "        mus_val = float(match.group(2))\n",
    "        labels.append((mua_val, mus_val))\n",
    "\n",
    "    labels = np.asarray(labels, dtype=np.float32)\n",
    "\n",
    "    # Save to CSV\n",
    "    label_df = pd.DataFrame(labels, columns=[\"mua\", \"mus\"])\n",
    "    label_df.to_csv(label_csv_path, index=False)\n",
    "\n",
    "    print(f\"Labels extracted and saved to: {label_csv_path}\")\n",
    "    print(f\"Total signals: {len(labels)}\")\n",
    "    print(f\"Example:\\n{label_df.head()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414ed861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator: \n",
    "    \"\"\"\n",
    "    Evaluates a trained model on a dataset and computes inversion accuracy metrics.  \n",
    "    Assumes that targets are 2D: (mua, mus)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model \n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.model.eval() # evaluation mode\n",
    "    \n",
    "    def evaluate(self, data_loader): \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for signals, labels in data_loader: \n",
    "                signals = signals.to(self.device)\n",
    "                labels = labels.to(self.device).float()\n",
    "                \n",
    "                preds = self.model(signals)\n",
    "                preds = preds.view_as(labels)\n",
    "\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds, dim = 0)\n",
    "        all_labels = torch.cat(all_labels, dim = 0)\n",
    "\n",
    "        # Compute errors \n",
    "        abs_err = torch.abs(all_preds - all_labels) # (N, 2)\n",
    "        sq_err = (all_preds - all_labels) ** 2\n",
    "\n",
    "        mae = abs_err.mean(dim = 0)\n",
    "        rmse = torch.sqrt(sq_err.mean(dim = 0))\n",
    "\n",
    "        metrics = {\n",
    "            \"MAE\": mae.numpy(), \n",
    "            \"RMSE\": rmse.numpy(),  \n",
    "            \"preds\": all_preds.numpy(), \n",
    "            \"lables\": all_labels.numpy()\n",
    "        }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b12fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels extracted and saved to: /Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_labels.csv\n",
      "Total signals: 400\n",
      "Example:\n",
      "        mua  mus\n",
      "0  0.005000  2.0\n",
      "1  0.005644  2.0\n",
      "2  0.006371  2.0\n",
      "3  0.007192  2.0\n",
      "4  0.008119  2.0\n",
      "signals: torch.Size([32, 3, 3000])\n",
      "labels: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "# Extract the labels from csv_path to label_csv_path\n",
    "\n",
    "csv_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_raw.csv\"\n",
    "label_csv_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_labels.csv\"\n",
    "extract_labels_from_dtof_csv(csv_path, label_csv_path)\n",
    "\n",
    "frame_length =21 \n",
    "order = 1\n",
    "\n",
    "# Load labels as a numpy array \n",
    "label_df = pd.read_csv(label_csv_path) # columns: [\"mua\", \"mus\"]\n",
    "labels_arr = label_df.values.astype(np.float32)\n",
    "\n",
    "# Create the DTOF dataset with labels\n",
    "\n",
    "dataset = DTOFDataset(\n",
    "    csv_path= csv_path,\n",
    "    labels = labels_arr, \n",
    "    window_length= frame_length, \n",
    "    polyorder= order, \n",
    "    eps = 1e-8, \n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "signals, labels = next(iter(loader))\n",
    "print(\"signals:\", signals.shape)\n",
    "print(\"labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc3698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 400\n",
      "Train samples: 320\n",
      "Val samples:   80\n"
     ]
    }
   ],
   "source": [
    "# Build dataset splits \n",
    "train_frac = 0.8 \n",
    "n_total = len(dataset)\n",
    "n_train = int(train_frac * n_total)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "generator = torch.Generator().manual_seed(42) #Â for reproducibility \n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, \n",
    "    [n_train, n_val], \n",
    "    generator = generator\n",
    ")\n",
    "\n",
    "print(\"Total samples:\", n_total)\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:  \", len(val_dataset))\n",
    "\n",
    "# Data Loaders \n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b210c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Epoch 1/ 20\n",
      "\n",
      "Epoch 2/ 20\n",
      "\n",
      "Epoch 3/ 20\n",
      "\n",
      "Epoch 4/ 20\n",
      "\n",
      "Epoch 5/ 20\n",
      "\n",
      "Epoch 6/ 20\n",
      "\n",
      "Epoch 7/ 20\n",
      "\n",
      "Epoch 8/ 20\n",
      "\n",
      "Epoch 9/ 20\n",
      "\n",
      "Epoch 10/ 20\n",
      "\n",
      "Epoch 11/ 20\n",
      "\n",
      "Epoch 12/ 20\n",
      "\n",
      "Epoch 13/ 20\n",
      "\n",
      "Epoch 14/ 20\n",
      "\n",
      "Epoch 15/ 20\n",
      "\n",
      "Epoch 16/ 20\n",
      "\n",
      "Epoch 17/ 20\n",
      "\n",
      "Epoch 18/ 20\n",
      "\n",
      "Epoch 19/ 20\n",
      "\n",
      "Epoch 20/ 20\n",
      "Train Loss: 0.1040\n",
      "Validation Loss: 0.1360\n",
      " -> Best validation loss so far, saving the model.\n",
      "MAE: [0.02461546 0.4895238 ]\n",
      "RMSE: [0.03028578 0.5288253 ]\n"
     ]
    }
   ],
   "source": [
    "# 1. Device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2. Model: instantiating the CNN and moving the model to the device before training\n",
    "model = Net(\n",
    "    in_channels=3, \n",
    "    input_length = dataset.T,\n",
    "    output_dim = 2 # predicting both (mua, mus)\n",
    ").to(device)\n",
    "\n",
    "# 3. Loss + optimizer \n",
    "loss_fn = torch.nn.MSELoss() # MSE error \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3) # learning rate of 0.001, decrease to 1e-4 if the training is unstable and increase to 3e-3 if training is too slow \n",
    "\n",
    "# 4. Train \n",
    "num_epochs = 20 \n",
    "\n",
    "train_model(\n",
    "    model = model, \n",
    "    train_loader = train_loader, \n",
    "    val_loader= val_loader, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= optimizer, \n",
    "    num_epochs= num_epochs, \n",
    "    device=device, \n",
    "    save_path= \"best_dtof_cnn.pth\"\n",
    "\n",
    ")\n",
    "\n",
    "# Forward pass with real DTOF batch, to verify the model pipeline from input -> output runs without shape errors\n",
    "outputs = model(signals)\n",
    "\n",
    "# Instantiate evaluator \n",
    "evaluator = ModelEvaluator(model, device)\n",
    "\n",
    "# Run evaluation on validation loader \n",
    "metrics = evaluator.evaluate(val_loader)\n",
    "\n",
    "print(\"MAE:\", metrics[\"MAE\"])\n",
    "print(\"RMSE:\", metrics[\"RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15bd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
