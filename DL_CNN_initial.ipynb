{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b73156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import random \n",
    "import torch \n",
    "from torch import nn \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "import pandas as pd \n",
    "import scipy \n",
    "from scipy.signal import savgol_filter \n",
    "import h5py\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Savitzky Golay filter parameters \n",
    "order = 1\n",
    "frame_length = 21\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b358256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTOFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DTOF dataset loaded from a MATLAB .mat file.\n",
    "\n",
    "    Expected variables in .mat:\n",
    "        X : (Nt, N) or (N, Nt)  reflectance DTOFs\n",
    "        y : (2, N)  or (N, 2)   [mua, mus]\n",
    "        t : (Nt,)              time vector (seconds, ~1e-12 resolution)\n",
    "\n",
    "    Preprocessing:\n",
    "        - convert t from seconds -> ns\n",
    "        - crop time axis to [0, crop_t_max] ns\n",
    "        - Savitzky–Golay smoothing\n",
    "        - clip small / negative values\n",
    "        - log-transform reflectance (Option B)\n",
    "        - optional per-trace standardisation\n",
    "        - channel construction:\n",
    "            * \"single\"\n",
    "            * \"early_mid_late\"\n",
    "            * \"hybrid_4ch\"\n",
    "\n",
    "    Returns:\n",
    "        signal: (C, T) tensor\n",
    "        label:  (2,) tensor  [mua, mus]  (raw, for now)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mat_path: str,\n",
    "            cfg: dict\n",
    "    ):\n",
    "            super().__init__()\n",
    "            self.cfg = cfg\n",
    "\n",
    "            # Load HDF5 .mat\n",
    "            with h5py.File(mat_path, \"r\") as f:\n",
    "                X = np.array(f[\"X\"], dtype=np.float32)\n",
    "                y = np.array(f[\"y\"], dtype=np.float32)\n",
    "                t = np.array(f[\"t\"], dtype=np.float32).squeeze()\n",
    "\n",
    "            # Ensure shapes \n",
    "            # X -> (N, Nt)\n",
    "            if X.shape[0] == t.shape[0]: \n",
    "                 X = X.T\n",
    "            X = X.astype(np.float32)\n",
    "\n",
    "            # y -> (N,2)\n",
    "            if y.shape[0] ==2: \n",
    "                 y = y.T\n",
    "            y = y.astype(np.float32)\n",
    "\n",
    "            if X.shape[1] != t.shape[0]:\n",
    "                 raise ValueError(\"Time axis length does not match DTOF length\")\n",
    "            \n",
    "            # time:seconds -> ns\n",
    "            t_ns = t * 1e9\n",
    "\n",
    "            # crop \n",
    "            crop_t_max = float(cfg[\"crop_t_max\"])  # ns\n",
    "            t_mask = (t_ns >= 0.0) & (t_ns <= crop_t_max)\n",
    "\n",
    "            if not np.any(t_mask):\n",
    "                raise ValueError(\n",
    "                    f\"Cropping removed all samples. \"\n",
    "                    f\"t_ns range=[{t_ns.min():.3g}, {t_ns.max():.3g}] ns\"\n",
    "                )\n",
    "\n",
    "            t_ns = t_ns[t_mask]\n",
    "            dtof = X[:, t_mask]          # (N, T)\n",
    "\n",
    "            N, T = dtof.shape\n",
    "\n",
    "            # Savitzky-Golay \n",
    "            sg_window = int(cfg[\"sg_window\"])\n",
    "            sg_order = int(cfg[\"sg_order\"])\n",
    "            \n",
    "            # enforce validity\n",
    "            if sg_window % 2 == 0:\n",
    "                sg_window += 1\n",
    "            if sg_window <= sg_order:\n",
    "                sg_window = sg_order + 2\n",
    "            if sg_window % 2 == 0:\n",
    "                sg_window += 1\n",
    "            if sg_window > T:\n",
    "                sg_window = T if T % 2 == 1 else T - 1\n",
    "\n",
    "            if sg_window >= 3:\n",
    "                dtof = savgol_filter(dtof, sg_window, sg_order, axis=1)\n",
    "            \n",
    "            # clip + log-transform\n",
    "            eps = float(cfg.get(\"eps\", 1e-12))\n",
    "            dtof[dtof < eps] = eps\n",
    "            dtof = np.log(dtof)\n",
    "\n",
    "\n",
    "            # channel construction\n",
    "            channels = self.build_channels(t_ns, dtof, cfg[\"channel_mode\"])\n",
    "\n",
    "            self.signals = torch.tensor(channels, dtype=torch.float32)\n",
    "            self.labels = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "            self.N, self.C, self.T = self.signals.shape\n",
    "\n",
    "    def build_channels(self, t_ns: np.ndarray, dtof: np.ndarray, mode: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Channel construction using ns time gates:\n",
    "            early: 0–0.5 ns\n",
    "            mid:   0.5–4 ns\n",
    "            late:  4–crop_t_max ns\n",
    "        \"\"\"\n",
    "        N, T = dtof.shape\n",
    "        crop_t_max = float(self.cfg[\"crop_t_max\"])\n",
    "\n",
    "        if mode == \"single\":\n",
    "            return dtof[:, None, :]  # (N,1,T)\n",
    "\n",
    "        early = ((t_ns >= 0.0) & (t_ns < 0.5)).astype(np.float32)\n",
    "        mid   = ((t_ns >= 0.5) & (t_ns < 4.0)).astype(np.float32)\n",
    "        late  = ((t_ns >= 4.0) & (t_ns <= crop_t_max)).astype(np.float32)\n",
    "\n",
    "        masks = np.stack([early, mid, late], axis=0)  # (3,T)\n",
    "\n",
    "        if mode == \"early_mid_late\":\n",
    "            return dtof[:, None, :] * masks[None, :, :]  # (N,3,T)\n",
    "\n",
    "        if mode == \"hybrid_4ch\":\n",
    "            full = dtof[:, None, :]\n",
    "            gated = dtof[:, None, :] * masks[None, :, :]\n",
    "            return np.concatenate([full, gated], axis=1)  # (N,4,T)\n",
    "\n",
    "        raise ValueError(f\"Unknown channel_mode: {mode}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.signals[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40d445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3000]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "matlab_path = \"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/dataset_homo_small.mat\"\n",
    "\n",
    "cfg = {\n",
    "    \"crop_t_max\": 6.0,        # ns\n",
    "    \"sg_window\": frame_length,\n",
    "    \"sg_order\": order,\n",
    "    \"eps\": 1e-12,\n",
    "    \"per_trace_standardise\": True,\n",
    "    \"channel_mode\": \"hybrid_4ch\",\n",
    "}\n",
    "\n",
    "ds = DTOFDataset(matlab_path, cfg)\n",
    "x, y = ds[0]\n",
    "print(x.shape, y.shape)  # (C, T), (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb24a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels = 3, input_length = 3000, output_dim = 2):\n",
    "        \"\"\"\n",
    "        CNN for 1D DTOF signals with 3 input channels (early / mid / late masks)\n",
    "        Blocks: [Conv1d -> BN -> ReLU -> MaxPool1d] x 3 -> Flatten -> FCs.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution blocks\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Compute the flattened feature size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_length)\n",
    "            feat = self._forward_features(dummy)\n",
    "            self.flatten_dim = feat.shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"Convolutional feature extractor followed by flatten.\"\"\"\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten to (batch, features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_channels = 3, time_points = 3000)\n",
    "        \"\"\"\n",
    "        x = self._forward_features(x) # (batch, flatten_dim)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a53976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, save_path = None): \n",
    "    \"\"\" \n",
    "    Train the CNN with a training + validation loop \n",
    "\n",
    "    Inputs: \n",
    "        model: instance of Net \n",
    "        train_loader: DataLoader for training set (yields signals, labels)\n",
    "        val_loader: DataLoader for validation set \n",
    "        loss_fun : loss function, e.g. nn.MSELoss()\n",
    "        optimiser: optimiser, e.g. torch.optim.Adam(...)\n",
    "        num_epochs: number of epochs to train \n",
    "        device: u\n",
    "        save_path: optional path to save best model, to use later when we develop more models (str or None)\n",
    "    \"\"\"\n",
    "    # Move model to device \n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        print(f\"\\nEpoch {epoch + 1}/ {num_epochs}\")\n",
    "\n",
    "        # TRAINING PHASE \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for signals, labels in train_loader: \n",
    "            # Move the batch to device \n",
    "            signals = signals.to(device) # (batch, 3, T = 3000)\n",
    "            labels = labels.to(device).float() # (batch,) or (batch, 1)\n",
    "\n",
    "            # Zero gradients \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass \n",
    "            preds = model(signals) # (batch, 1) or (batch, )\n",
    "            preds = preds.view_as(labels) # reshape the predictions to have the same shape as labels \n",
    "\n",
    "            # Loss\n",
    "            loss = loss_fn(preds, labels)\n",
    "            \n",
    "            # Backward + update \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signals, labels in val_loader: \n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(signals)\n",
    "            preds = preds.view_as(labels)\n",
    "\n",
    "            loss = loss_fn(preds, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss: \n",
    "        print(\" -> Best validation loss so far, saving the model.\")\n",
    "        best_val_loss = val_loss\n",
    "        if save_path is not None: \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bb93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, target, test_size = 0.2, shuffle = True, random_state = None):\n",
    "    \"\"\"\n",
    "    Splits the data and target lists into training and validation subsets. \n",
    "    \"\"\"\n",
    "\n",
    "    if len(data) != len(target): \n",
    "        raise ValueError(\"Data and target must have the same length.\")\n",
    "    \n",
    "    if shuffle: \n",
    "        if random_state is not None: \n",
    "            random.seed(random_state)\n",
    "        pairs = list(zip(data, target))\n",
    "        random.shuffle(pairs)\n",
    "        data, target = zip(*pairs)\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "\n",
    "    return (\n",
    "        data[:split_idx], # X_train\n",
    "        data[split_idx:], # X_val \n",
    "        target[:split_idx], # y_train\n",
    "        target[split_idx:] # y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbd8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_from_dtof_csv(csv_path, label_csv_path):\n",
    "    \"\"\"\n",
    "    Extract (mua, mus) labels from DTOF CSV column headers and save to a new CSV file.\n",
    "\n",
    "    Input: \n",
    "        csv_path : str\n",
    "            Path to the large DTOF CSV file (first column = time_ns, others = DTOFs).\n",
    "        label_csv_path : str\n",
    "            Where to save the generated labels CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # DTOF columns start from index 1\n",
    "    dtof_columns = df.columns[1:]\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for col in dtof_columns:\n",
    "        col_clean = col.strip()\n",
    "\n",
    "        # Regex matches text like \"mua: 0.015 mus: 0.75\"\n",
    "        match = re.search(r\"mua:\\s*([0-9.]+)\\s+mus:\\s*([0-9.]+)\", col_clean)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse mua/mus from column '{col}'\")\n",
    "\n",
    "        mua_val = float(match.group(1))\n",
    "        mus_val = float(match.group(2))\n",
    "        labels.append((mua_val, mus_val))\n",
    "\n",
    "    labels = np.asarray(labels, dtype=np.float32)\n",
    "\n",
    "    # Save to CSV\n",
    "    label_df = pd.DataFrame(labels, columns=[\"mua\", \"mus\"])\n",
    "    label_df.to_csv(label_csv_path, index=False)\n",
    "\n",
    "    print(f\"Labels extracted and saved to: {label_csv_path}\")\n",
    "    print(f\"Total signals: {len(labels)}\")\n",
    "    print(f\"Example:\\n{label_df.head()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414ed861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator: \n",
    "    \"\"\"\n",
    "    Evaluates a trained model on a dataset and computes inversion accuracy metrics.  \n",
    "    Assumes that targets are 2D: (mua, mus)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model \n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.model.eval() # evaluation mode\n",
    "    \n",
    "    def evaluate(self, data_loader): \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for signals, labels in data_loader: \n",
    "                signals = signals.to(self.device)\n",
    "                labels = labels.to(self.device).float()\n",
    "                \n",
    "                preds = self.model(signals)\n",
    "                preds = preds.view_as(labels)\n",
    "\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds, dim = 0)\n",
    "        all_labels = torch.cat(all_labels, dim = 0)\n",
    "\n",
    "        # Compute errors \n",
    "        abs_err = torch.abs(all_preds - all_labels) # (N, 2)\n",
    "        sq_err = (all_preds - all_labels) ** 2\n",
    "\n",
    "        mae = abs_err.mean(dim = 0)\n",
    "        rmse = torch.sqrt(sq_err.mean(dim = 0))\n",
    "\n",
    "        metrics = {\n",
    "            \"MAE\": mae.numpy(), \n",
    "            \"RMSE\": rmse.numpy(),  \n",
    "            \"preds\": all_preds.numpy(), \n",
    "            \"lables\": all_labels.numpy()\n",
    "        }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41037b45",
   "metadata": {},
   "source": [
    "INITIAL DATA: Converting .mat file containing DTOFs into 2 .csv files: labels + DTOF data\n",
    "\n",
    "Data description: Labels csv saved as DTOFs_Homo_labels.csv and DTOFs_Homo_raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b12fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels extracted and saved to: /Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_labels.csv\n",
      "Total signals: 400\n",
      "Example:\n",
      "        mua  mus\n",
      "0  0.005000  2.0\n",
      "1  0.005644  2.0\n",
      "2  0.006371  2.0\n",
      "3  0.007192  2.0\n",
      "4  0.008119  2.0\n",
      "signals: torch.Size([32, 3, 3000])\n",
      "labels: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "# Extract the labels from csv_path to label_csv_path\n",
    "\n",
    "csv_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_raw.csv\"\n",
    "label_csv_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_labels.csv\"\n",
    "extract_labels_from_dtof_csv(csv_path, label_csv_path)\n",
    "\n",
    "frame_length =21 \n",
    "order = 1\n",
    "\n",
    "# Load labels as a numpy array \n",
    "label_df = pd.read_csv(label_csv_path) # columns: [\"mua\", \"mus\"]\n",
    "labels_arr = label_df.values.astype(np.float32)\n",
    "\n",
    "# Create the DTOF dataset with labels\n",
    "\n",
    "dataset = DTOFDataset(\n",
    "    csv_path= csv_path,\n",
    "    labels = labels_arr, \n",
    "    window_length= frame_length, \n",
    "    polyorder= order, \n",
    "    eps = 1e-8, \n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "signals, labels = next(iter(loader))\n",
    "print(\"signals:\", signals.shape)\n",
    "print(\"labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels from csv_path to label_csv_path\n",
    "\n",
    "csv_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_raw.csv\"\n",
    "label_csv_path = r\"/Users/lydialichen/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3/Research Project in Biomedical Engineering/Code/Pre-obtained data/DTOFs_Homo_labels.csv\"\n",
    "extract_labels_from_dtof_csv(csv_path, label_csv_path)\n",
    "\n",
    "frame_length =21 \n",
    "order = 1\n",
    "\n",
    "# Load labels as a numpy array \n",
    "label_df = pd.read_csv(label_csv_path) # columns: [\"mua\", \"mus\"]\n",
    "labels_arr = label_df.values.astype(np.float32)\n",
    "\n",
    "# Create the DTOF dataset with labels\n",
    "\n",
    "dataset = DTOFDataset(\n",
    "    csv_path= csv_path,\n",
    "    labels = labels_arr, \n",
    "    window_length= frame_length, \n",
    "    polyorder= order, \n",
    "    eps = 1e-8, \n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "signals, labels = next(iter(loader))\n",
    "print(\"signals:\", signals.shape)\n",
    "print(\"labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539dc3d9",
   "metadata": {},
   "source": [
    "FINAL DATA: Converting .mat file containing DTOFs into 2 .csv files: labels + DTOF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc3698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 400\n",
      "Train samples: 320\n",
      "Val samples:   80\n"
     ]
    }
   ],
   "source": [
    "# Build dataset splits \n",
    "train_frac = 0.8 \n",
    "n_total = len(dataset)\n",
    "n_train = int(train_frac * n_total)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "generator = torch.Generator().manual_seed(42) # for reproducibility \n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, \n",
    "    [n_train, n_val], \n",
    "    generator = generator\n",
    ")\n",
    "\n",
    "print(\"Total samples:\", n_total)\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:  \", len(val_dataset))\n",
    "\n",
    "# Data Loaders \n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2. Model: instantiating the CNN and moving the model to the device before training\n",
    "model = Net(\n",
    "    in_channels=3, \n",
    "    input_length = dataset.T,\n",
    "    output_dim = 2 # predicting both (mua, mus)\n",
    ").to(device)\n",
    "\n",
    "# 3. Loss + optimizer \n",
    "loss_fn = torch.nn.MSELoss() # MSE error \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3) # learning rate of 0.001, decrease to 1e-4 if the training is unstable and increase to 3e-3 if training is too slow \n",
    "\n",
    "# 4. Train \n",
    "num_epochs = 20 \n",
    "\n",
    "train_model(\n",
    "    model = model, \n",
    "    train_loader = train_loader, \n",
    "    val_loader= val_loader, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= optimizer, \n",
    "    num_epochs= num_epochs, \n",
    "    device=device, \n",
    "    save_path= \"best_dtof_cnn.pth\"\n",
    "\n",
    ")\n",
    "\n",
    "# Forward pass with real DTOF batch, to verify the model pipeline from input -> output runs without shape errors\n",
    "outputs = model(signals)\n",
    "\n",
    "# Instantiate evaluator \n",
    "evaluator = ModelEvaluator(model, device)\n",
    "\n",
    "# Run evaluation on validation loader \n",
    "metrics = evaluator.evaluate(val_loader)\n",
    "\n",
    "print(\"MAE:\", metrics[\"MAE\"])\n",
    "print(\"RMSE:\", metrics[\"RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15bd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
